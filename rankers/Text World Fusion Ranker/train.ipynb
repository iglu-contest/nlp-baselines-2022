{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import AutoModelForMultipleChoice\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import gc\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numpy.random import Generator,default_rng\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from scipy.stats import rankdata\n",
    "import torch\n",
    "import transformers\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import log_loss\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertTokenizer, RobertaTokenizer, BartTokenizer, AutoTokenizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,WeightedRandomSampler\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from transformers import (\n",
    "    RobertaForSequenceClassification,\n",
    "    BartForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "emuyxyavx2Kq"
   },
   "source": [
    "## Proprocessing - Tokenize the text data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1659926987543,
     "user": {
      "displayName": "Dipam Chakraborty",
      "userId": "04496869744334527762"
     },
     "user_tz": -480
    },
    "id": "LhoERwqDgAG0"
   },
   "outputs": [],
   "source": [
    "def get_tensor_dataset(df, tokenizer):\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    topic_ids = []\n",
    "\n",
    "    for count, item in tqdm(\n",
    "        enumerate(\n",
    "            zip(\n",
    "                df[\"GameId\"],\n",
    "                df[\"bylevel_color_context\"],  # df[\"nonspatial_color_context\"],\n",
    "                df[\"InputInstruction\"],\n",
    "                df[\"IsInstructionClear\"],\n",
    "            )\n",
    "        ),\n",
    "        total=len(df),\n",
    "        desc=\"Tokenizing data\",\n",
    "    ):\n",
    "        z, w, x, y = item\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            w,\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=\"max_length\",  # use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'`\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_dict[\"input_ids\"])\n",
    "\n",
    "        if \"token_type_ids\" in encoded_dict:\n",
    "            token_type_ids.append(encoded_dict[\"token_type_ids\"])\n",
    "\n",
    "        attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "        labels.append(y)\n",
    "\n",
    "        topic_ids.append(z)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return TensorDataset(input_ids, attention_masks, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304,
     "referenced_widgets": [
      "61dc9814bbfd4e81ada1e57a6794e5d9",
      "6f321bece4d54226ad3db809bde4cd4e",
      "4b1fbe85127d4fce871b4c0f458d4caf",
      "88d292e97b534caf9838af5fabd8e540",
      "8ae76991239d410cacddb929c1d735df",
      "17d2a8fdb0ce42089433a0619b8bc377",
      "a5fd56d0815149ab9fba81694ee2abc0",
      "d726c20b8f644fa4ac5377947858f6b4",
      "f26d3641d21545868b01052ff2da32d6",
      "9d8d70b4fde84533b467778c64ee2925",
      "3b22025d2e034bf290207b9c0dbcd77f",
      "f46af155d2914e4d9f062f51d869d1e9",
      "67ba0ca897d64f07885d8deffea6e33f",
      "e3d9f3b105854da1a79aae69cfa55f19",
      "2bfe0f794fe144aa81187c4affe9f7ab",
      "a43d6f520fe441ec94ed76864e6ad686",
      "158c3bef95684e64a15dc20cbc0e6883",
      "0fe02fa28f11441fa2e56b8a50e5bb97",
      "282675c3d54e4c7f97bf688a12eedd34",
      "767c97ecb8974a5db20253f46320560a",
      "7c8eae5b06ec43118f99a850374175ec",
      "454945d525b8452e82c50882b1e409b6",
      "d4a7e94247d34e98b09686a425ca946c",
      "e6997bb0ab9b45338f99f97968c035a7",
      "f96be4566e8441399e1a9a21755bfdf6",
      "1876f748227547aba4398b1e4b399199",
      "623a01dafad04664b67ebc194c987a12",
      "a210597827134638bb830849be952b8c",
      "b5a61fafc66e4943b98ed21973f81e4f",
      "e869f95a3a674aea98c1e0c8835a6bf2",
      "bb415c3be154471fb714854f59da1d92",
      "03fef63aa66a49309fb2436854d15d24",
      "ec0ac8e334864dde9c94ec8ae5289635",
      "e3011a737a594d1f9ddcf1b43fed5f6b",
      "881bbeb5c4ab482a91c2160db4e4e88a",
      "a39165728b2245cbad191c61418a4ab5",
      "e988e0b8cd524b01b31a6b2166b6e046",
      "74c46ff68ce34edc907309a92c0b0c54",
      "bd0af9a490d14ca8bb205e321314b859",
      "e47cb091546d4c2cacae5f86e491197d",
      "e7ae34cd46ca4509bd38404c3daa3538",
      "b4281ccb093047d8af28894544ef80de",
      "4563aede5c6c4cf78dd25b934c846bba",
      "fdedfcc0b56c40b3826f46da2a5f5597",
      "4471b53588f54822abe07419be2fc95e",
      "cd43d555a10340ac8c5eff9535474501",
      "d578e757c116407cae78f9b72885258c",
      "7997b15718674b6a81f1306e25177e55",
      "e6e23b79a2bb49b5ab83221aa0525b3c",
      "c79348e0c978443794c9b83d476a4a1f",
      "330e9455dae8406ea6cf48463b4bd373",
      "536dcbf35dd74c4aafd518e4aad725fe",
      "35fa9d72c9ac488892631ced49b6a08e",
      "2b8235324e9043849ae3ce48310a10e0",
      "5419b989589044ffae226242e0bbd2e2"
     ]
    },
    "executionInfo": {
     "elapsed": 9353,
     "status": "ok",
     "timestamp": 1659926997215,
     "user": {
      "displayName": "Dipam Chakraborty",
      "userId": "04496869744334527762"
     },
     "user_tz": -480
    },
    "id": "ZZoC_BXxgAsU",
    "outputId": "2093a0cd-d7d4-4a38-ad94-75ca1f03114b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path='public_data/clarifying_questions_train.csv'\n",
    "df = pd.read_csv(data_path, sep=\",\")\n",
    "df = pd.read_csv(\"public_data/clarifying_questions_train_split.csv\")\n",
    "df_test = pd.read_csv(\"public_data/clarifying_questions_test.csv\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "train_posranker_selection = pd.read_pickle(\"../my_data/full_db_posranker_selection.pk\")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "# tokenizer.convert_ids_to_tokens(tokenized_swag['train'][0]['input_ids'][2])\n",
    "# tokenized_swag['train'][0]['input_ids']\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    PaddingStrategy,\n",
    ")\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)]\n",
    "            for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # print(batch)\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"public_data/clarifying_questions_w_context.csv\")\n",
    "train_posranker_selection = train_posranker_selection.merge(\n",
    "    df[[\"GameId\", \"nonspatial_color_context\",'bylevel_color_context']],how='left'\n",
    ")\n",
    "dc = DataCollatorForMultipleChoice(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_info = {}\n",
    "\n",
    "for key, df in train_posranker_selection.groupby(\"GameId\"):\n",
    "    qlist = df[\"proposed_q\"].to_list()\n",
    "    realq = df[\"qtrue\"].iloc[0]\n",
    "    games_info[key] = {\n",
    "        \"rank_score\": df[\"score\"].to_list(),\n",
    "        \"clarifying_questions\": df[\"ClarifyingQuestion\"].to_list(),\n",
    "        \"proposed_qid\": qlist,\n",
    "        \"real_q\": realq,\n",
    "        \"instruction\": df['bylevel_color_context'].iloc[0]\n",
    "        + tokenizer.sep_token\n",
    "        + df[\"InputInstruction\"].iloc[0],\n",
    "        #'context':,\n",
    "        \"label\": qlist.index(realq) if realq in qlist else None,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_around_label(realq_id,qlist,num=25):\n",
    "    top = min(len(qlist),realq_id+(num//2))\n",
    "    return qlist[top-num:top]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_colour_name_map = {\n",
    "    # voxelworld's colour id : iglu colour id\n",
    "    0: \"air\",\n",
    "    1: \"blue\",\n",
    "    6: \"yellow\",\n",
    "    2: \"green\",\n",
    "    4: \"orange\",\n",
    "    5: \"purple\",\n",
    "    3: \"red\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_colours =[val for key,val in block_colour_name_map.items() if val !='air']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gen = default_rng(seed=42)\n",
    "gen.permutation(original_colours)\n",
    "cur_per = gen.permutation(original_colours)\n",
    "no_per = {key:key for key in original_colours}\n",
    "per_dict ={ key:val for key,val in zip(original_colours,cur_per)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_full_words(text, dic):\n",
    "    for i, j in dic.items():\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, j, text)\n",
    "        # r\"\\b%s\\b\"% enables replacing by whole word matches only\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "npqs=120\n",
    "def generate_augmented_games_info():\n",
    "    \n",
    "    original_colours =[val for key,val in block_colour_name_map.items() if val !='air']\n",
    "    no_per = {key:key for key in original_colours}\n",
    "    c_i_q_sets =[]\n",
    "    for i in range(5):\n",
    "        if i==0:\n",
    "            cur_per=no_per\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            ar_per = gen.permutation(original_colours)\n",
    "            cur_per ={ key:val for key,val in zip(original_colours,ar_per)}\n",
    "            \n",
    "            \n",
    "        games_info = {}\n",
    "        \n",
    "        for key, df in train_posranker_selection.groupby(\"GameId\"):\n",
    "            qlist = df[\"proposed_q\"].to_list()\n",
    "            realq = df[\"qtrue\"].iloc[0]\n",
    "            clariq = df[\"ClarifyingQuestion\"].to_list()\n",
    "            rank_score =df[\"score\"].to_list()\n",
    "\n",
    "            original_true_index = qlist.index(realq) \n",
    "            if original_true_index<npqs:\n",
    "                qlist = qlist[:npqs]\n",
    "                rank_score = rank_score[:npqs],\n",
    "                clariq = clariq[:npqs]\n",
    "            else:\n",
    "                if np.random.choice([False,True]):\n",
    "                    left_qs = np.random.randint(10,70)\n",
    "                    qlist = qlist[:left_qs]+get_questions_around_label(original_true_index,qlist,num=npqs-left_qs)\n",
    "                    rank_score = rank_score[:left_qs]+get_questions_around_label(original_true_index,rank_score,num=npqs-left_qs)\n",
    "                    clariq = clariq[:left_qs]+get_questions_around_label(original_true_index,clariq,num=npqs-left_qs)\n",
    "                else:\n",
    "\n",
    "                    left_qs = np.random.randint(10,70)\n",
    "                    qlist = get_questions_around_label(original_true_index,qlist,num=npqs-left_qs) + qlist[:left_qs]\n",
    "                    rank_score = get_questions_around_label(original_true_index,rank_score,num=npqs-left_qs) + rank_score[:left_qs]\n",
    "                    clariq = get_questions_around_label(original_true_index,clariq,num=npqs-left_qs) + clariq[:left_qs]\n",
    "\n",
    "            games_info[key] = {\n",
    "                \"rank_score\": rank_score,\n",
    "                \"clarifying_questions\":[replace_full_words(q,cur_per) for q in clariq],\n",
    "                \"proposed_qid\": qlist,\n",
    "                \"real_q\": realq,\n",
    "                \"instruction\": replace_full_words(df['bylevel_color_context'].iloc[0]\n",
    "                + tokenizer.sep_token\n",
    "                + df[\"InputInstruction\"].iloc[0],cur_per),\n",
    "                #'context':,\n",
    "                \"label\": qlist.index(realq) if realq in qlist else None,\n",
    "            }\n",
    "\n",
    "        c_i_q_sets.append(games_info)\n",
    "    \n",
    "    return c_i_q_sets\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_i_q_sets = generate_augmented_games_info()\n",
    "games_info = c_i_q_sets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_with_target = {\n",
    "    game: features\n",
    "    for game, features in games_info.items()\n",
    "    if features[\"label\"] is not None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos_questions = npqs\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [\n",
    "        examples[\"instruction\"]\n",
    "    ] * num_pos_questions  # [[context] * len(num_pos_questions) for context in examples[\"instruction\"]]\n",
    "\n",
    "    second_sentences = examples[\"clarifying_questions\"]\n",
    "\n",
    "    # first_sentences = sum(first_sentences, [])\n",
    "    # second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences, second_sentences, truncation=True, padding=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        k: [v[i : i + num_pos_questions] for i in range(0, len(v), num_pos_questions)]\n",
    "        for k, v in tokenized_examples.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "preproc = preprocess_function(games_info[\"CQ-game-1117\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_games_with_target(c_i_q_sets):\n",
    "    import multiprocess\n",
    "    augmented_games_with_targets=[]\n",
    "    for data in c_i_q_sets:\n",
    "     \n",
    "\n",
    "        dataset_games_with_targets = list(data.values())\n",
    "\n",
    "        dataset_games_tok = [\n",
    "            {k: item[k] for k in [\"clarifying_questions\", \"instruction\"]}\n",
    "            for item in dataset_games_with_targets\n",
    "        ]\n",
    "\n",
    "        with multiprocess.Pool(processes=4) as pool:\n",
    "\n",
    "            res = pool.map(preprocess_function, dataset_games_tok)\n",
    "\n",
    "            # pool.join()\n",
    "        res[0]\n",
    "        # @TOD batch it\n",
    "        # tokenized_swag = swag.map(preprocess_function, batched=True)\n",
    "        \n",
    "\n",
    "        for i in range(len(res)):\n",
    "\n",
    "            dataset_games_with_targets[i].update(res[i])\n",
    "\n",
    "        augmented_games_with_targets.append(dataset_games_with_targets)   \n",
    "\n",
    "    return augmented_games_with_targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "augwt =  generate_games_with_target(c_i_q_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPleChoiceRankerAugmentedTrainDataset(Dataset):\n",
    "    def __init__(self,tokenized_array_list):\n",
    "\n",
    "        self.data = tokenized_array_list\n",
    "        self.columns =  ['label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data_idx =np.random.randint(0,len(self.data))\n",
    "\n",
    "        row =self.data[data_idx][index]\n",
    "\n",
    "        for col in ['input_ids','token_type_ids', 'attention_mask']:\n",
    "\n",
    "            if len(row[col])==1:\n",
    "                row[col] = row[col][0]#[torch.Tensor(val) for val in row[col][0] ]\n",
    "            #row[col] = torch.Tensor(row[col])\n",
    "\n",
    "        #print(row)\n",
    "        \n",
    "        \n",
    "        return {col: row[col] for col in self.columns}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPleChoiceRankerDataset(Dataset):\n",
    "    def __init__(self,tokenized_array):\n",
    "\n",
    "        self.data = tokenized_array\n",
    "        self.columns =  ['label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        row =self.data[index]\n",
    "\n",
    "        for col in ['input_ids','token_type_ids', 'attention_mask']:\n",
    "\n",
    "            if len(row[col])==1:\n",
    "                row[col] = row[col][0]#[torch.Tensor(val) for val in row[col][0] ]\n",
    "            #row[col] = torch.Tensor(row[col])\n",
    "\n",
    "        #print(row)\n",
    "        \n",
    "        \n",
    "        return {col: row[col] for col in self.columns}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiPleChoiceRankerAugmentedTrainDataset(augwt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    dataset, collate_fn=DataCollatorForMultipleChoice(tokenizer), batch_size=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb = True\n",
    "    competition = \"PPPM\"\n",
    "    _wandb_kernel = \"nakama\"\n",
    "    debug = False\n",
    "    apex = True\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "    model = \"microsoft/deberta-v3-large\"\n",
    "    scheduler = \"linear\"  #'cosine' # ['linear', 'cosine','constant','plateau']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 1.0\n",
    "    num_warmup_steps = 0\n",
    "    epochs = 5\n",
    "    encoder_lr = 7e-5#5e-5  # lr 5e-5 accumulation 4 batch 8 wd 0.003produces overfittin in the first epch\n",
    "    decoder_lr = 3e-5  # 3e-5# 8e-6\n",
    "    min_lr = 1e-7\n",
    "    eps = 1e-8\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 36\n",
    "    fc_dropout = 0.2\n",
    "    target_size = 1\n",
    "    max_len = 320\n",
    "    weight_decay = 0.003#0.01  # 0.002#0.015#0.005#0.001#0.00005\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 60  # 200#500 #10#100#1000#150#1000\n",
    "    seed = 42\n",
    "    n_fold = 5\n",
    "    trn_fold = [0, 1, 2, 3, 4]\n",
    "    train = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "accumulation_steps =8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"lr\": encoder_lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"lr\": encoder_lr,\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        # {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "        #     'lr': decoder_lr, 'weight_decay':  weight_decay}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    # if cfg.scheduler == 'linear':\n",
    "    #     scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    #         optimizer, num_warmup_steps=0.2 * num_train_steps, num_training_steps=num_train_steps\n",
    "    #     )\n",
    "    if cfg.scheduler == \"linear\":\n",
    "        scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == \"cosine\":\n",
    "        scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0.2 * num_train_steps,\n",
    "            num_training_steps=int(num_train_steps * 1.1),\n",
    "            num_cycles=cfg.num_cycles,\n",
    "        )\n",
    "    elif cfg.scheduler == \"constant\":\n",
    "        scheduler = transformers.get_constant_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=0.3 * num_train_steps\n",
    "        )\n",
    "\n",
    "    elif cfg.scheduler == \"plateau\":\n",
    "        scheduler = transformers.ReduceLronPlateau(\n",
    "            optimizer, num_warmup_steps=0.3 * num_train_steps\n",
    "        )\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "#scheduler = get_scheduler(CFG, optimizer, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 5\n",
    "lr=CFG.encoder_lr\n",
    "model_hug = \"microsoft/deberta-v3-base\"\n",
    "CFG.weight_decay=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        adv_param=\"weight\",\n",
    "        adv_lr=1e-2,\n",
    "        adv_eps=9e-4,\n",
    "        start_epoch=0,\n",
    "        adv_step=1,\n",
    "        scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "    def attack_backward(self, x, y, attention_mask,epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save() \n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step() \n",
    "            with torch.cuda.amp.autocast():\n",
    "                tr_logits = self.model(x,attention_mask)['logits']\n",
    "                adv_loss =  self.loss(tr_logits,y)\n",
    "                adv_loss = adv_loss.mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "            \n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        #e=1e-3\n",
    "        #e=1e-3\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self,):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"deberta_cv_full_recall_25_25_env_on_full_db120_augmentcolorper_wd5e-3_AWP_continued_awp2_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ranker(model_name,model,dl,dl_val,scheduler,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        epochs,\n",
    "        lr,\n",
    "        fold,):\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    awp = AWP(model,\n",
    "      optimizer,\n",
    "      # adv_lr=args.adv_lr,\n",
    "      # adv_eps=args.adv_eps,\n",
    "      start_epoch=1,\n",
    "      scaler=scaler\n",
    "         )\n",
    "\n",
    "\n",
    "    general_val_loss = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        #  if DEBUG and (epoch>0): break\n",
    "        start_time = time.time()\n",
    "        avg_loss = 0.0\n",
    "        model.train()\n",
    "        tbar = tqdm(dl)\n",
    "        loss_list = []\n",
    "        val_loss_list = []\n",
    "        model.train()\n",
    "\n",
    "        # freeze_embd(net,i)\n",
    "        total_rr=[]\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        for step, batch in enumerate(tbar):\n",
    "\n",
    "            batch = {key: part.to(device) for key, part in batch.items()}\n",
    "            # input_ids = data['ids']\n",
    "            # input_masks = data['mask']\n",
    "            # tag_ids = data['tag_ids']\n",
    "            # targets = data['targets']#.long().view(-1).cuda()\n",
    "\n",
    "            # input_ids,input_masks = checkpoint(net,**dict(ids=input_ids,mask=input_masks))\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])[\n",
    "                    \"logits\"\n",
    "                ]\n",
    "                # pred = checkpoint(net,(input_ids,input_masks))\n",
    "\n",
    "                # loss = loss_fn(pred.softmax(dim=1), targets)\n",
    "                loss = loss_fn(pred, batch[\"labels\"])\n",
    "\n",
    "            # loss.backward()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            if epoch > -1:\n",
    "                if loss <2.3: #0.78\n",
    "                    awp.attack_backward(x=batch[\"input_ids\"],y=batch['labels'],attention_mask=batch[\"attention_mask\"],epoch=epoch)\n",
    "                del loss\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                # awp.attack_backward(x=input_ids,y=targets,attention_mask=input_masks,tag_ids=tag_ids,epoch=epoch)\n",
    "\n",
    "            else:\n",
    "\n",
    "                del loss\n",
    "\n",
    "            # grad_norm = torch.nn.utils.clip_grad_norm_(net.parameters(), CFG.max_grad_norm)\n",
    "            if step % accumulation_steps == 0 or step == len(tbar) - 1:\n",
    "                # Unscales the gradients of optimizer's assigned params in-place\n",
    "                #    scaler.unscale_(optimizer)\n",
    "\n",
    "                #                   # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "                #    torch.nn.utils.clip_grad_norm_(net.parameters(), CFG.max_grad_norm)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                # optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "\n",
    "            avg_loss = np.round(np.mean(loss_list), 4)\n",
    "            cur_lrs = [float(param_group[\"lr\"]) for param_group in optimizer.param_groups]\n",
    "            \n",
    "            pred_val = pred.detach().cpu().numpy()\n",
    "            btarget_val = batch[\"labels\"].detach().cpu().numpy()\n",
    "            cur_rr = 1/np.take_along_axis(arr=(pred_val.shape[1]+1 - rankdata(pred_val,axis=1)),indices=btarget_val[:,None],axis=1) \n",
    "                \n",
    "            total_rr.append(cur_rr)\n",
    "            tbar.set_description(\n",
    "                f\"Train Epoch {epoch + 1} Loss: {avg_loss} RR:{np.mean(total_rr)} lr: { cur_lrs[0]:4f},{ cur_lrs[1]:4f}\"\n",
    "            )\n",
    "\n",
    "            del pred, batch\n",
    "            gc.collect()\n",
    "        total_rr=[]\n",
    "        torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            avg_val_loss = 0.0\n",
    "            tbar_val = tqdm(dl_val)\n",
    "            for step, batch in enumerate(tbar_val):\n",
    "                batch = {key: part.to(device) for key, part in batch.items()}\n",
    "\n",
    "                # input_ids = data['ids'].cuda()\n",
    "                # input_masks = data['mask'].cuda()\n",
    "                # tag_ids = data['tag_ids'].cuda()\n",
    "                # targets = data['targets'].cuda()#.long().view(-1).cuda()\n",
    "\n",
    "                pred = model(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])[\n",
    "                    \"logits\"\n",
    "                ]\n",
    "                # pred = checkpoint(net,(input_ids,input_masks))\n",
    "\n",
    "                # loss = loss_fn(pred.softmax(dim=1), targets)\n",
    "                loss = loss_fn(pred, batch[\"labels\"])\n",
    "                \n",
    "                val_loss_list.append(loss.detach().cpu().item())\n",
    "                pred_val = pred.detach().cpu().numpy()\n",
    "                btarget_val = batch[\"labels\"].detach().cpu().numpy()\n",
    "                cur_rr = 1/np.take_along_axis(arr=(pred_val.shape[1]+1 - rankdata(pred_val,axis=1)),indices=btarget_val[:,None],axis=1) \n",
    "                \n",
    "                total_rr.append(cur_rr)\n",
    "                avg_val_loss = np.round(np.mean(val_loss_list), 4)\n",
    "                tbar_val.set_description(f\" Val Epoch {epoch + 1} Loss: {avg_val_loss} RR:{np.mean(cur_rr)}\")\n",
    "            print(avg_val_loss,np.mean(np.concatenate(total_rr)))\n",
    "\n",
    "            model.save_pretrained(f\"saved_model/ranker_{model_name}_{lr}lr_{EPOCHS}/e_{epoch}/f{fold}\") #_f{fold}\")\n",
    "            if (avg_val_loss/avg_loss)>1.5:\n",
    "                break  \n",
    "            \n",
    "        # if True:\n",
    "\n",
    "        #     scheduler2.step(avg_val_loss)\n",
    "\n",
    "        general_val_loss.append(avg_val_loss)\n",
    "        # with open('roberta_large_epoch_%s_fold_%s.pkl' % (epoch, i), 'wb') as f: pkl.dump(net.to(torch.device(\"cpu\")), f)\n",
    "        # torch.save(net.state_dict(),'deberta_base5_epoch_%s_fold_%s.pkl' % (epoch, i))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " 5 -- 7e-05 -- 2022-10-07 23:40:24.572378\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForMultipleChoice: ['mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/238 [00:00<?, ?it/s]/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2339: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train Epoch 1 Loss: 2.0629 RR:0.6116862420128752 lr: 0.000056,0.000056: 100%|█████████▉| 237/238 [15:44<00:03,  4.00s/it]/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "Train Epoch 1 Loss: 2.0549 RR:[[0.59115103]\n",
      " [0.63105421]\n",
      "Train Epoch 1 Loss: 2.0549 RR:[[0.59115103]█████████▉| 237/238 [15:45<00:03,  4.00s/it]\n",
      " [0.63105421]\n",
      "Train Epoch 1 Loss: 2.0549 RR:[[0.59115103]██████████| 238/238 [15:47<00:00,  3.69s/it]\n",
      " [0.63105421]\n",
      " [0.6177482 ]] lr: 0.000055,0.000055: 100%|██████████| 238/238 [15:47<00:00,  3.98s/it]\n",
      " Val Epoch 1 Loss: 2.1451 RR:1.0: 100%|██████████| 60/60 [01:17<00:00,  1.29s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1451 0.5561356671473406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2 Loss: 1.7204 RR:[[0.65010574]37386 lr: 0.000041,0.000041: 100%|█████████▉| 237/238 [21:27<00:06,  6.49s/it]\n",
      " [0.67594069]\n",
      "Train Epoch 2 Loss: 1.7204 RR:[[0.65010574]█████████▉| 237/238 [21:30<00:06,  6.49s/it]\n",
      " [0.67594069]\n",
      "Train Epoch 2 Loss: 1.7204 RR:[[0.65010574]██████████| 238/238 [21:31<00:00,  5.73s/it]\n",
      " [0.67594069]\n",
      " [0.65152813]] lr: 0.000041,0.000041: 100%|██████████| 238/238 [21:31<00:00,  5.43s/it]\n",
      " Val Epoch 2 Loss: 2.1409 RR:1.0: 100%|██████████| 60/60 [01:16<00:00,  1.27s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1409 0.5623073522130575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3 Loss: 1.628 RR:[[0.6810877 ]279928 lr: 0.000026,0.000026: 100%|█████████▉| 237/238 [21:31<00:05,  5.94s/it]\n",
      " [0.67412274]\n",
      "Train Epoch 3 Loss: 1.628 RR:[[0.6810877 ]|█████████▉| 237/238 [21:33<00:05,  5.94s/it]\n",
      " [0.67412274]\n",
      "Train Epoch 3 Loss: 1.628 RR:[[0.6810877 ]|██████████| 238/238 [21:34<00:00,  5.09s/it]\n",
      " [0.67412274]\n",
      " [0.68605225]] lr: 0.000026,0.000026: 100%|██████████| 238/238 [21:34<00:00,  5.44s/it]\n",
      " Val Epoch 3 Loss: 1.9691 RR:1.0: 100%|██████████| 60/60 [01:16<00:00,  1.27s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9691 0.6031437540687263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4 Loss: 1.4975 RR:[[0.67763249]85405 lr: 0.000012,0.000012: 100%|█████████▉| 237/238 [21:34<00:05,  6.00s/it]\n",
      " [0.70178255]\n",
      "Train Epoch 4 Loss: 1.4975 RR:[[0.67763249]█████████▉| 237/238 [21:36<00:05,  6.00s/it]\n",
      " [0.70178255]\n",
      "Train Epoch 4 Loss: 1.4975 RR:[[0.67763249]██████████| 238/238 [21:37<00:00,  5.16s/it]\n",
      " [0.70178255]\n",
      " [0.70363157]] lr: 0.000011,0.000011: 100%|██████████| 238/238 [21:37<00:00,  5.45s/it]\n",
      " Val Epoch 4 Loss: 1.9394 RR:1.0: 100%|██████████| 60/60 [01:13<00:00,  1.22s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9394 0.602536162331024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5 Loss: 1.4469 RR:[[0.7255627 ]18202 lr: 0.000000,0.000000: 100%|█████████▉| 237/238 [21:06<00:05,  5.39s/it]\n",
      " [0.70384711]\n",
      "Train Epoch 5 Loss: 1.4469 RR:[[0.7255627 ]█████████▉| 237/238 [21:09<00:05,  5.39s/it]\n",
      " [0.70384711]\n",
      "Train Epoch 5 Loss: 1.4469 RR:[[0.7255627 ]██████████| 238/238 [21:10<00:00,  5.01s/it]\n",
      " [0.70384711]\n",
      " [0.68171598]] lr: 0.000000,0.000000: 100%|██████████| 238/238 [21:10<00:00,  5.34s/it]\n",
      " Val Epoch 5 Loss: 1.9972 RR:1.0: 100%|██████████| 60/60 [01:18<00:00,  1.31s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9972 0.6006828773432443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForMultipleChoice: ['mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 Loss: 2.0576 RR:[[0.60132416]95017 lr: 0.000056,0.000056: 100%|█████████▉| 237/238 [15:29<00:03,  3.93s/it]\n",
      " [0.59309935]\n",
      "Train Epoch 1 Loss: 2.0576 RR:[[0.60132416]█████████▉| 237/238 [15:31<00:03,  3.93s/it]\n",
      " [0.59309935]\n",
      "Train Epoch 1 Loss: 2.0576 RR:[[0.60132416]██████████| 238/238 [15:32<00:00,  3.69s/it]\n",
      " [0.59309935]\n",
      " [0.60567473]] lr: 0.000055,0.000055: 100%|██████████| 238/238 [15:32<00:00,  3.92s/it]\n",
      " Val Epoch 1 Loss: 2.1255 RR:0.125: 100%|██████████| 60/60 [01:16<00:00,  1.28s/it]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1255 0.5861348059243212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2 Loss: nan RR:[[0.6534558 ]3652 lr: 0.000041,0.000041: 100%|█████████▉| 237/238 [21:47<00:05,  5.71s/it]   \n",
      " [0.67209204]\n",
      "Train Epoch 2 Loss: nan RR:[[0.6534558 ]0%|█████████▉| 237/238 [21:49<00:05,  5.71s/it]\n",
      " [0.67209204]\n",
      "Train Epoch 2 Loss: nan RR:[[0.6534558 ]0%|██████████| 238/238 [21:50<00:00,  4.84s/it]\n",
      " [0.67209204]\n",
      " [0.67017346]] lr: 0.000041,0.000041: 100%|██████████| 238/238 [21:50<00:00,  5.51s/it]\n",
      " Val Epoch 2 Loss: 2.0378 RR:1.0: 100%|██████████| 60/60 [01:18<00:00,  1.31s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0378 0.6146198344967224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3 Loss: 1.6266 RR:[[0.67319963]37749 lr: 0.000026,0.000026: 100%|█████████▉| 237/238 [22:32<00:06,  6.20s/it]\n",
      " [0.65098937]\n",
      "Train Epoch 3 Loss: 1.6266 RR:[[0.67319963]█████████▉| 237/238 [22:33<00:06,  6.20s/it]\n",
      " [0.65098937]\n",
      "Train Epoch 3 Loss: 1.6266 RR:[[0.67319963]██████████| 238/238 [22:34<00:00,  5.11s/it]\n",
      " [0.65098937]\n",
      " [0.71275614]] lr: 0.000026,0.000026: 100%|██████████| 238/238 [22:34<00:00,  5.69s/it]\n",
      " Val Epoch 3 Loss: 1.9413 RR:1.0: 100%|██████████| 60/60 [01:15<00:00,  1.26s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9413 0.6182663470164755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4 Loss: 1.5978 RR:[[0.71486853]9791 lr: 0.000012,0.000012: 100%|█████████▉| 237/238 [22:07<00:06,  6.13s/it] \n",
      " [0.65106469]\n",
      "Train Epoch 4 Loss: 1.5978 RR:[[0.71486853]█████████▉| 237/238 [22:10<00:06,  6.13s/it]\n",
      " [0.65106469]\n",
      "Train Epoch 4 Loss: 1.5978 RR:[[0.71486853]██████████| 238/238 [22:11<00:00,  5.32s/it]\n",
      " [0.65106469]\n",
      " [0.71807208]] lr: 0.000011,0.000011: 100%|██████████| 238/238 [22:11<00:00,  5.59s/it]\n",
      " Val Epoch 4 Loss: 1.9397 RR:1.0: 100%|██████████| 60/60 [01:19<00:00,  1.33s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9397 0.6209675442534071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5 Loss: 1.4258 RR:[[0.73816155]10524 lr: 0.000000,0.000000: 100%|█████████▉| 237/238 [24:00<00:05,  5.84s/it]\n",
      " [0.69735946]\n",
      "Train Epoch 5 Loss: 1.4258 RR:[[0.73816155]█████████▉| 237/238 [24:02<00:05,  5.84s/it]\n",
      " [0.69735946]\n",
      "Train Epoch 5 Loss: 1.4258 RR:[[0.73816155]██████████| 238/238 [24:03<00:00,  5.18s/it]\n",
      " [0.69735946]\n",
      " [0.716293  ]] lr: 0.000000,0.000000: 100%|██████████| 238/238 [24:03<00:00,  6.07s/it]\n",
      " Val Epoch 5 Loss: 1.9534 RR:1.0: 100%|██████████| 60/60 [01:27<00:00,  1.46s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9534 0.6204510540515905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForMultipleChoice: ['mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 Loss: 1.9103 RR:[[0.59083084]55929 lr: 0.000056,0.000056: 100%|█████████▉| 237/238 [17:59<00:04,  4.92s/it]\n",
      " [0.62897914]\n",
      "Train Epoch 1 Loss: 1.9103 RR:[[0.59083084]█████████▉| 237/238 [18:01<00:04,  4.92s/it]\n",
      " [0.62897914]\n",
      "Train Epoch 1 Loss: 1.9103 RR:[[0.59083084]██████████| 238/238 [18:02<00:00,  4.54s/it]\n",
      " [0.62897914]\n",
      " [0.62658246]] lr: 0.000055,0.000055: 100%|██████████| 238/238 [18:02<00:00,  4.55s/it]\n",
      " Val Epoch 1 Loss: 2.9576 RR:0.125: 100%|██████████| 60/60 [01:30<00:00,  1.50s/it]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9576 0.5261710620933313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForMultipleChoice: ['mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 Loss: 1.9458 RR:[[0.62355847]63444 lr: 0.000056,0.000056: 100%|█████████▉| 237/238 [18:15<00:04,  4.03s/it]\n",
      " [0.66712275]\n",
      "Train Epoch 1 Loss: 1.9458 RR:[[0.62355847]█████████▉| 237/238 [18:17<00:04,  4.03s/it]\n",
      " [0.66712275]\n",
      "Train Epoch 1 Loss: 1.9458 RR:[[0.62355847]██████████| 238/238 [18:18<00:00,  3.69s/it]\n",
      " [0.66712275]\n",
      " [0.59285673]] lr: 0.000055,0.000055: 100%|██████████| 238/238 [18:18<00:00,  4.62s/it]\n",
      " Val Epoch 1 Loss: 2.6674 RR:0.06666666666666667: 100%|██████████| 60/60 [01:28<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6674 0.5130106374084629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2 Loss: 1.6764 RR:[[0.65146608]20855 lr: 0.000041,0.000041: 100%|█████████▉| 237/238 [21:36<00:06,  6.51s/it]\n",
      " [0.67521816]\n",
      "Train Epoch 2 Loss: 1.6764 RR:[[0.65146608]█████████▉| 237/238 [21:38<00:06,  6.51s/it]\n",
      " [0.67521816]\n",
      "Train Epoch 2 Loss: 1.6764 RR:[[0.65146608]██████████| 238/238 [21:39<00:00,  5.45s/it]\n",
      " [0.67521816]\n",
      " [0.68716794]] lr: 0.000041,0.000041: 100%|██████████| 238/238 [21:39<00:00,  5.46s/it]\n",
      " Val Epoch 2 Loss: 2.7221 RR:0.0625: 100%|██████████| 60/60 [01:19<00:00,  1.32s/it]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7221 0.49825882428737656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForMultipleChoice: ['mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 Loss: nan RR:[[0.58366303]37433 lr: 0.000056,0.000056: 100%|█████████▉| 237/238 [16:25<00:04,  4.53s/it]   \n",
      " [0.61451255]\n",
      "Train Epoch 1 Loss: nan RR:[[0.58366303]0%|█████████▉| 237/238 [16:27<00:04,  4.53s/it]\n",
      " [0.61451255]\n",
      "Train Epoch 1 Loss: nan RR:[[0.58366303]0%|██████████| 238/238 [16:28<00:00,  4.13s/it]\n",
      " [0.61451255]\n",
      " [0.57579456]] lr: 0.000055,0.000055: 100%|██████████| 238/238 [16:28<00:00,  4.15s/it]\n",
      " Val Epoch 1 Loss: 2.2065 RR:1.0: 100%|██████████| 60/60 [01:27<00:00,  1.45s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2065 0.5927998709174923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2 Loss: 1.8429 RR:[[0.67435958]25372 lr: 0.000041,0.000041: 100%|█████████▉| 237/238 [24:42<00:08,  8.47s/it]\n",
      " [0.62921609]\n",
      "Train Epoch 2 Loss: 1.8429 RR:[[0.67435958]█████████▉| 237/238 [24:45<00:08,  8.47s/it]\n",
      " [0.62921609]\n",
      "Train Epoch 2 Loss: 1.8429 RR:[[0.67435958]██████████| 238/238 [24:46<00:00,  7.34s/it]\n",
      " [0.62921609]\n",
      " [0.66652602]] lr: 0.000041,0.000041: 100%|██████████| 238/238 [24:46<00:00,  6.25s/it]\n",
      " Val Epoch 2 Loss: 2.2466 RR:0.5: 100%|██████████| 60/60 [01:39<00:00,  1.66s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2466 0.5514552409069716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3 Loss: 1.7221 RR:[[0.66610212]55247 lr: 0.000026,0.000026: 100%|█████████▉| 237/238 [24:03<00:05,  5.72s/it]\n",
      " [0.67458865]\n",
      "Train Epoch 3 Loss: 1.7221 RR:[[0.66610212]█████████▉| 237/238 [24:05<00:05,  5.72s/it]\n",
      " [0.67458865]\n",
      "Train Epoch 3 Loss: 1.7221 RR:[[0.66610212]██████████| 238/238 [24:06<00:00,  5.05s/it]\n",
      " [0.67458865]\n",
      " [0.63725004]] lr: 0.000026,0.000026: 100%|██████████| 238/238 [24:06<00:00,  6.08s/it]\n",
      " Val Epoch 3 Loss: 2.2023 RR:1.0: 100%|██████████| 60/60 [01:25<00:00,  1.42s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2023 0.5865265946115953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4 Loss: 1.6075 RR:[[0.70022568]90678 lr: 0.000012,0.000012: 100%|█████████▉| 237/238 [23:40<00:05,  5.67s/it]\n",
      " [0.67103999]\n",
      "Train Epoch 4 Loss: 1.6075 RR:[[0.70022568]█████████▉| 237/238 [23:41<00:05,  5.67s/it]\n",
      " [0.67103999]\n",
      "Train Epoch 4 Loss: 1.6075 RR:[[0.70022568]██████████| 238/238 [23:43<00:00,  4.84s/it]\n",
      " [0.67103999]\n",
      " [0.66006892]] lr: 0.000011,0.000011: 100%|██████████| 238/238 [23:43<00:00,  5.98s/it]\n",
      " Val Epoch 4 Loss: 2.1007 RR:0.5: 100%|██████████| 60/60 [01:26<00:00,  1.44s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1007 0.5941716876879327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5 Loss: 1.5857 RR:[[0.69827662]11481 lr: 0.000000,0.000000: 100%|█████████▉| 237/238 [24:04<00:06,  6.10s/it]\n",
      " [0.67579433]\n",
      "Train Epoch 5 Loss: 1.5857 RR:[[0.69827662]█████████▉| 237/238 [24:07<00:06,  6.10s/it]\n",
      " [0.67579433]\n",
      "Train Epoch 5 Loss: 1.5857 RR:[[0.69827662]██████████| 238/238 [24:08<00:00,  5.38s/it]\n",
      " [0.67579433]\n",
      " [0.69411055]] lr: 0.000000,0.000000: 100%|██████████| 238/238 [24:08<00:00,  6.09s/it]\n",
      " Val Epoch 5 Loss: 2.1149 RR:0.5: 100%|██████████| 60/60 [01:36<00:00,  1.60s/it]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1149 0.592733582107103\n"
     ]
    }
   ],
   "source": [
    "print(\"============================================================\")\n",
    "print(f\" {epochs} -- {lr} -- {datetime.datetime.now()}\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "# with open(f'{model_name}_train.pkl', 'rb') as f:\n",
    "#     train_dataset = pickle.load(f)\n",
    "skf = KFold(FOLDS, random_state=42, shuffle=True)\n",
    "for fold, (tr_id, val_id) in enumerate(skf.split(dataset_for_cv[0])):\n",
    "    # dfdev.to_csv('public_data/clarifying_questions_val_w_context.csv', index=False)\n",
    "    \n",
    "    dataset_tr = MultiPleChoiceRankerAugmentedTrainDataset([dat[tr_id] for dat in dataset_for_cv])\n",
    "    dataset_val = MultiPleChoiceRankerDataset(dataset_for_cv[0][val_id])\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_hug)\n",
    "\n",
    "    dl = DataLoader(\n",
    "    dataset_tr,\n",
    "    collate_fn=DataCollatorForMultipleChoice(tokenizer, max_length=320),\n",
    "    batch_size= 3,#8,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "    dl_val = DataLoader(\n",
    "        dataset_val,\n",
    "        collate_fn=DataCollatorForMultipleChoice(tokenizer, max_length=320),\n",
    "        batch_size=3,#8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    device = \"cuda:0\"\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(f\"saved_model/ranker_{old_model_name}_{lr}lr_{epochs}/e_4/f{fold}/pytorch_model.bin\"))\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "    model,\n",
    "    encoder_lr=CFG.encoder_lr,\n",
    "    decoder_lr=CFG.decoder_lr,\n",
    "    weight_decay=CFG.weight_decay,\n",
    ")\n",
    "\n",
    "\n",
    "    optimizer = Adam(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "\n",
    "    # optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01}, {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    num_train_steps = int(EPOCHS * len(dl) / accumulation_steps)\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.3 * num_train_optimization_steps, num_training_steps = num_train_optimization_steps)\n",
    "    print(num_train_steps)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    # Train Model\n",
    "    train_ranker(\n",
    "        model_name,\n",
    "        model,\n",
    "        dl,\n",
    "        dl_val,\n",
    "        scheduler,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        epochs,\n",
    "        lr,\n",
    "        fold,\n",
    "    )\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('iglu2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bce01a44bbe2ca620061f41984838f4961c7f78d841ffeab292f7643abbc5513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
