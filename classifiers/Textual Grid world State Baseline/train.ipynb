{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"JYFBB8pe1QSR"},"source":["# Train a BERT classifier - \"When to ask clarifying question\" â“\n","# Textual Grid world State Baseline\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from dataclasses import dataclass\n","import os\n","import datetime\n","import pickle\n","import numpy as np\n","import random\n","import numpy as np\n","from numpy.random import Generator,default_rng\n","import torch\n","import pandas as pd\n","from torch.utils.data import TensorDataset\n","from transformers import BertTokenizer, RobertaTokenizer, BartTokenizer, AutoTokenizer\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,WeightedRandomSampler\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import (\n","    BertForSequenceClassification,\n","    AdamW,\n","    get_linear_schedule_with_warmup,\n",")\n","from transformers import (\n","    RobertaForSequenceClassification,\n","    BartForSequenceClassification,\n","    AutoModelForSequenceClassification,\n",")\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import KFold, StratifiedKFold\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7wvKRaoFyGBY"},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":324,"status":"ok","timestamp":1659926984565,"user":{"displayName":"Dipam Chakraborty","userId":"04496869744334527762"},"user_tz":-480},"id":"RNBDvVaSgMM3"},"outputs":[],"source":["model_name = \"derbertav3_base_oversampling\"\n","model_hug = \"microsoft/deberta-v3-base\"\n","max_seq_length = 320\n","batch_size = 16\n","epoch = 15\n","lr = 1e-5\n","seed_val = 42"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"emuyxyavx2Kq"},"source":["## Proprocessing - Tokenize the text data for BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":518,"status":"ok","timestamp":1659926987543,"user":{"displayName":"Dipam Chakraborty","userId":"04496869744334527762"},"user_tz":-480},"id":"LhoERwqDgAG0"},"outputs":[],"source":["def get_tensor_dataset(df, tokenizer):\n","\n","    # Tokenize all of the sentences and map the tokens to thier word IDs.\n","    input_ids = []\n","    token_type_ids = []\n","    attention_masks = []\n","    labels = []\n","    topic_ids = []\n","\n","    for count, item in tqdm(\n","        enumerate(\n","            zip(\n","                df[\"GameId\"],\n","                df[\"bylevel_color_context\"],  # df[\"nonspatial_color_context\"],\n","                df[\"InputInstruction\"],\n","                df[\"IsInstructionClear\"],\n","            )\n","        ),\n","        total=len(df),\n","        desc=\"Tokenizing data\",\n","    ):\n","        z, w, x, y = item\n","        encoded_dict = tokenizer.encode_plus(\n","            w,\n","            x,\n","            add_special_tokens=True,\n","            max_length=max_seq_length,\n","            padding=\"max_length\",  # use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'`\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        input_ids.append(encoded_dict[\"input_ids\"])\n","\n","        if \"token_type_ids\" in encoded_dict:\n","            token_type_ids.append(encoded_dict[\"token_type_ids\"])\n","\n","        attention_masks.append(encoded_dict[\"attention_mask\"])\n","        labels.append(y)\n","\n","        topic_ids.append(z)\n","\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels)\n","    return TensorDataset(input_ids, attention_masks, labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["('saved_tokenizer_microsoft/deberta-v3-base/tokenizer_config.json',\n"," 'saved_tokenizer_microsoft/deberta-v3-base/special_tokens_map.json',\n"," 'saved_tokenizer_microsoft/deberta-v3-base/spm.model',\n"," 'saved_tokenizer_microsoft/deberta-v3-base/added_tokens.json',\n"," 'saved_tokenizer_microsoft/deberta-v3-base/tokenizer.json')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_hug)\n","tokenizer.save_pretrained(f\"saved_tokenizer_{model_hug}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304,"referenced_widgets":["61dc9814bbfd4e81ada1e57a6794e5d9","6f321bece4d54226ad3db809bde4cd4e","4b1fbe85127d4fce871b4c0f458d4caf","88d292e97b534caf9838af5fabd8e540","8ae76991239d410cacddb929c1d735df","17d2a8fdb0ce42089433a0619b8bc377","a5fd56d0815149ab9fba81694ee2abc0","d726c20b8f644fa4ac5377947858f6b4","f26d3641d21545868b01052ff2da32d6","9d8d70b4fde84533b467778c64ee2925","3b22025d2e034bf290207b9c0dbcd77f","f46af155d2914e4d9f062f51d869d1e9","67ba0ca897d64f07885d8deffea6e33f","e3d9f3b105854da1a79aae69cfa55f19","2bfe0f794fe144aa81187c4affe9f7ab","a43d6f520fe441ec94ed76864e6ad686","158c3bef95684e64a15dc20cbc0e6883","0fe02fa28f11441fa2e56b8a50e5bb97","282675c3d54e4c7f97bf688a12eedd34","767c97ecb8974a5db20253f46320560a","7c8eae5b06ec43118f99a850374175ec","454945d525b8452e82c50882b1e409b6","d4a7e94247d34e98b09686a425ca946c","e6997bb0ab9b45338f99f97968c035a7","f96be4566e8441399e1a9a21755bfdf6","1876f748227547aba4398b1e4b399199","623a01dafad04664b67ebc194c987a12","a210597827134638bb830849be952b8c","b5a61fafc66e4943b98ed21973f81e4f","e869f95a3a674aea98c1e0c8835a6bf2","bb415c3be154471fb714854f59da1d92","03fef63aa66a49309fb2436854d15d24","ec0ac8e334864dde9c94ec8ae5289635","e3011a737a594d1f9ddcf1b43fed5f6b","881bbeb5c4ab482a91c2160db4e4e88a","a39165728b2245cbad191c61418a4ab5","e988e0b8cd524b01b31a6b2166b6e046","74c46ff68ce34edc907309a92c0b0c54","bd0af9a490d14ca8bb205e321314b859","e47cb091546d4c2cacae5f86e491197d","e7ae34cd46ca4509bd38404c3daa3538","b4281ccb093047d8af28894544ef80de","4563aede5c6c4cf78dd25b934c846bba","fdedfcc0b56c40b3826f46da2a5f5597","4471b53588f54822abe07419be2fc95e","cd43d555a10340ac8c5eff9535474501","d578e757c116407cae78f9b72885258c","7997b15718674b6a81f1306e25177e55","e6e23b79a2bb49b5ab83221aa0525b3c","c79348e0c978443794c9b83d476a4a1f","330e9455dae8406ea6cf48463b4bd373","536dcbf35dd74c4aafd518e4aad725fe","35fa9d72c9ac488892631ced49b6a08e","2b8235324e9043849ae3ce48310a10e0","5419b989589044ffae226242e0bbd2e2"]},"executionInfo":{"elapsed":9353,"status":"ok","timestamp":1659926997215,"user":{"displayName":"Dipam Chakraborty","userId":"04496869744334527762"},"user_tz":-480},"id":"ZZoC_BXxgAsU","outputId":"2093a0cd-d7d4-4a38-ad94-75ca1f03114b"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","data_path='public_data/clarifying_questions_train.csv'\n","df = pd.read_csv(data_path, sep=\",\")\n","data_path = \"public_data/clarifying_questions_train.csv\"\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hkwuePD8yTOK"},"source":["## Train Classifier ðŸ‹ï¸â€â™€ï¸"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","import importlib\n","from preprocess.tools import block_tools\n","from preprocess.gridworld_preprocess import complete_df_with_grid_state\n","\n","importlib.reload(preprocess.gridworld_preprocess)\n","sys.path.append(\"../\")\n","df = complete_df_with_grid_state(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from preprocess.tools.block_tools import (\n","    transform_block,\n","    count_block_colors,\n","    create_context_colour_count,\n",")\n","importlib.reload(block_tools)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["0       [(0, -1, 0, 5), (0, 0, 0, 5), (0, 1, 0, 5), (0...\n","1       [(-2, 2, -1, 1), (-1, 0, -1, 5), (-1, 1, -1, 5...\n","2       [(-2, 2, -1, 1), (-1, 0, -1, 5), (-1, 1, -1, 5...\n","3       [(-1, -1, 1, 5), (-1, 0, 1, 5), (-1, 1, 1, 5),...\n","4       [(-5, -1, -5, 3), (-5, -1, -4, 3), (-5, -1, 4,...\n","                              ...                        \n","6823    [(-4, -1, -2, 2), (-4, -1, -1, 2), (0, -1, -1,...\n","6824    [(3, -1, 1, 2), (3, 1, 1, 2), (4, -1, 1, 2), (...\n","6825    [(-5, -1, 0, 3), (-4, -1, 0, 3), (-4, 0, 0, 3)...\n","6826    [(-4, -1, -1, 2), (0, -1, -1, 1), (0, 0, -1, 1...\n","6827    [(-4, 0, 0, 5), (-2, 0, 0, 5), (0, 0, 0, 6), (...\n","Name: blocks, Length: 6828, dtype: object"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df[\"blocks\"] = df[\"blocks\"].apply(\n","    lambda blocks: [transform_block(block) for block in blocks]\n",")\n","df[\"blocks\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["0                                         {'purple': 13}\n","1                     {'blue': 2, 'purple': 9, 'red': 2}\n","2                     {'blue': 2, 'purple': 9, 'red': 2}\n","3                                          {'purple': 5}\n","4       {'red': 4, 'purple': 4, 'yellow': 4, 'green': 4}\n","                              ...                       \n","6823                             {'green': 2, 'blue': 4}\n","6824                                        {'green': 8}\n","6825                                          {'red': 7}\n","6826                             {'green': 1, 'blue': 4}\n","6827              {'purple': 2, 'yellow': 4, 'green': 9}\n","Name: blocks, Length: 6828, dtype: object"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["color_block_freq = df[\"blocks\"].apply(count_block_colors)\n","color_block_freq\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["0       [{'purple': 1}, {'purple': 1}, {'purple': 2}, ...\n","1       [{'purple': 3, 'red': 1}, {'purple': 3, 'red':...\n","2       [{'purple': 3, 'red': 1}, {'purple': 3, 'red':...\n","3       [{'purple': 1}, {'purple': 1}, {'purple': 1}, ...\n","4       [{'red': 3, 'purple': 3, 'yellow': 3, 'green':...\n","                              ...                        \n","6823    [{'green': 2, 'blue': 1}, {'blue': 1}, {'blue'...\n","6824    [{'green': 3}, {'green': 1}, {'green': 3}, {'g...\n","6825                             [{'red': 4}, {'red': 3}]\n","6826    [{'green': 1, 'blue': 1}, {'blue': 1}, {'blue'...\n","6827    [{'green': 3}, {'purple': 2, 'yellow': 2, 'gre...\n","Name: blocks, Length: 6828, dtype: object"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["color_block_freq_byL = df[\"blocks\"].apply(block_tools.get_color_counter_by_level)\n","color_block_freq_byL\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["count    6828.000000\n","mean      101.247803\n","std        25.457803\n","min        58.000000\n","25%        83.000000\n","50%       106.000000\n","75%       110.000000\n","max       161.000000\n","Name: nonspatial_color_context, dtype: float64"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df[\"nonspatial_color_context\"] = color_block_freq.apply(block_tools.create_context_colour_count)\n","df[\"nonspatial_color_context\"].apply(len).describe()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["count    6828.000000\n","mean       52.279145\n","std        27.529863\n","min         9.000000\n","25%        32.000000\n","50%        50.000000\n","75%        68.000000\n","max       125.000000\n","Name: bylevel_color_context, dtype: float64"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["df[\"bylevel_color_context\"] = color_block_freq_byL.apply(\n","    block_tools.create_context_colour_by_height_level\n",")\n","\n","df[\"bylevel_color_context\"].str.split(' ').apply(len).describe()\n","df.to_csv(\"public_data/clarifying_questions_w_context.csv\", index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df,df_test = train_test_split(df,test_size=0.1,stratify=df['IsInstructionClear'],random_state=42)\n","df = pd.read_csv(\"public_data/clarifying_questions_train_split.csv\")\n","df_test = pd.read_csv(\"public_data/clarifying_questions_test.csv\")\n","df_or = pd.read_csv(\"public_data/clarifying_questions_w_context.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df.merge(df_or[['GameId',\n","       'InputInstructionWithGameID', 'pos', 'look', 'blocks',\n","       'nonspatial_color_context', 'bylevel_color_context']],on='GameId',how='left')\n","\n","df_test = df_test.merge(df_or[['GameId',\n","       'InputInstructionWithGameID', 'pos', 'look', 'blocks',\n","       'nonspatial_color_context', 'bylevel_color_context']],on='GameId',how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","def eval_result( labels,preds):\n","    \"\"\" Calculate the accuracy, f1, precision, recall of our predictions vs labels\n","    \"\"\"\n","    if len(preds.shape) ==2:\n","        y_pred = np.argmax(preds, axis=1).flatten()\n","    else:\n","        y_pred=preds\n","    y_true = labels.flatten()\n","\n","    precision = precision_score(y_true, y_pred,zero_division=1)\n","    recall = recall_score(y_true, y_pred,zero_division=1)\n","    f1 = f1_score(y_true, y_pred, average='macro',zero_division=1)\n","    accuracy = np.sum(y_pred == y_true) / len(y_true) \n","\n","    return (precision, recall, f1, accuracy)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1659926997216,"user":{"displayName":"Dipam Chakraborty","userId":"04496869744334527762"},"user_tz":-480},"id":"oapcUHhzhO-k"},"outputs":[],"source":["def train_model(\n","    model_name,\n","    model,\n","    train_dataloader,\n","    val_dataloader,\n","    scheduler,\n","    optimizer,\n","    criterion,\n","    epochs,\n","    lr,\n","    fold=0,\n","):\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","\n","    else:\n","        device = torch.device(\"cpu\")\n","    print(device)\n","    scaler = torch.cuda.amp.GradScaler()\n","    # For each epoch...\n","    for epoch in range(epochs):\n","\n","        total_train_loss = 0\n","        train_n_correct = 0\n","        nb_tr_examples = 0\n","\n","        model.train()\n","\n","        for _, batch in tqdm(\n","            enumerate(train_dataloader),\n","            total=len(train_dataloader),\n","            desc=f\"Train epoch {epoch+1}/{epochs}\",\n","        ):\n","\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            model.zero_grad()\n","            with torch.cuda.amp.autocast():\n","                result = model(b_input_ids, attention_mask=b_input_mask)\n","\n","                loss = criterion(result.logits, b_labels)\n","            #loss.backward()\n","            scaler.scale(loss).backward()\n","            total_train_loss += loss.item()\n","\n","            #optimizer.step()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            \n","            scheduler.step()\n","\n","            logits = result.logits.detach().cpu().numpy()\n","            label_ids = b_labels.to(\"cpu\").numpy()\n","            _, _, _, accuracy = eval_result(label_ids,logits)\n","            train_n_correct += accuracy\n","            nb_tr_examples += b_input_ids.size(0)\n","\n","        avg_train_loss = total_train_loss / len(train_dataloader)\n","        train_acc = train_n_correct / len(train_dataloader)\n","\n","        print(\n","            \"Epoch [{}/{}], Train Loss: {:.4f}, Train Accuracy: {:.4f} \".format(\n","                epoch + 1, epochs, avg_train_loss, train_acc\n","            )\n","        )\n","\n","        model.eval()\n","\n","        test_results = []\n","        test_labels = []\n","        total_val_loss=0\n","        test_results_predicted_lavels = []\n","        test_results_predicted_lavels2 = []\n","        test_results_predicted_lavels3 = []\n","        for batch in tqdm(val_dataloader, desc=\"Eval model\"):\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            with torch.no_grad():\n","                result = model(\n","                    b_input_ids,\n","                    attention_mask=b_input_mask,\n","                    labels=b_labels,\n","                    return_dict=True,\n","                )\n","                loss = criterion(result.logits, b_labels)\n","            total_val_loss += loss.item()\n","            logits = torch.softmax(result.logits,axis=1)#result.logits\n","            logits = logits.detach().cpu().numpy()\n","            test_results.extend(logits.tolist())\n","            tmp = np.asarray(logits.tolist())\n","\n","            # test_results_predicted_lavels.extend(np.max(tmp,axis=1).flatten())\n","\n","            test_results_predicted_lavels.extend(tmp[:,1]>0.15)\n","            test_results_predicted_lavels2.extend(tmp[:,1]>0.5)\n","            test_results_predicted_lavels3.extend(tmp[:,1]>0.7)\n","            \n","            avg_val_loss = total_val_loss / len(val_dataloader)\n","            label_ids = b_labels.to(\"cpu\").numpy()\n","            test_labels.extend(label_ids)\n","\n","        print(\n","            classification_report(\n","                np.asarray(test_labels), np.asarray(test_results_predicted_lavels)\n","            )\n","        )\n","        (precision, recall, f1, accuracy) = eval_result(\n","            np.asarray(test_labels), np.asarray(test_results_predicted_lavels)\n","        )\n","\n","        print(\n","            \"Epoch [{}/{}], Test Loss: {:.4f},Test Precision: {:.4f}, Test Recall: {:.4f}, Test Macro F1: {:.4f}, Test Accuracy: {:.4f} \".format(\n","                epoch + 1, epochs, avg_val_loss, precision, recall, f1, accuracy\n","            )\n","        )\n","\n","\n","        print(classification_report( np.asarray(test_labels), np.asarray(test_results_predicted_lavels2)))\n","        (precision, recall, f1, accuracy) = eval_result(np.asarray(test_labels), np.asarray(test_results_predicted_lavels2))\n","\n","        print('Test Precision: {:.4f}, Test Recall: {:.4f}, Test Macro F1: {:.4f}, Test Accuracy: {:.4f} ' .format(precision, recall, f1, accuracy))\n","\n","        (precision, recall, f1, accuracy) = eval_result(np.asarray(test_labels), np.asarray(test_results_predicted_lavels3))\n","\n","        print('Test Precision: {:.4f}, Test Recall: {:.4f}, Test Macro F1: {:.4f}, Test Accuracy: {:.4f} ' .format(precision, recall, f1, accuracy))\n","\n","        model.save_pretrained(f\"saved_model/{model_name}_{epoch}/{epochs}e_{lr}lr_f{fold}\")\n","\n","    print(\"Training complete!\")\n","\n","    # model.save_pretrained(f\"drive/MyDrive/IGLU-cq-data/{model_name}_{epochs}e_{lr}lr\")\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["batch_size = 8\n","epoch = 7\n","FOLDS = 5\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["lr = 1e-5\n","batch_size=24"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["df2 =df.copy()"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'df2' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/felipe/minerl2022/iglu/nlp/repo/iglu-2022-clariq-nlp-starter-kit/nb/deberta_context1_classifier_ranker.ipynb Cell 70\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/felipe/minerl2022/iglu/nlp/repo/iglu-2022-clariq-nlp-starter-kit/nb/deberta_context1_classifier_ranker.ipynb#Y126sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df2[\u001b[39m\"\u001b[39m\u001b[39mIsInstructionClear\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df2\u001b[39m.\u001b[39mIsInstructionClear\u001b[39m.\u001b[39mreplace(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felipe/minerl2022/iglu/nlp/repo/iglu-2022-clariq-nlp-starter-kit/nb/deberta_context1_classifier_ranker.ipynb#Y126sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mYes\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/felipe/minerl2022/iglu/nlp/repo/iglu-2022-clariq-nlp-starter-kit/nb/deberta_context1_classifier_ranker.ipynb#Y126sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     )\n","\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"]}],"source":["df2[\"IsInstructionClear\"] = df2.IsInstructionClear.replace(\n","        {\"Yes\": 0, \"No\": 1}\n","    )"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"text/plain":["array([1, 1, 1, ..., 1, 1, 1])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["df2[\"IsInstructionClear\"].apply(lambda x: 4 if x==1 else 1).values"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.01973891258239746,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"Tokenizing data","rate":null,"total":6145,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"a70db5774097456e871711cf965edc3a","version_major":2,"version_minor":0},"text/plain":["Tokenizing data:   0%|          | 0/6145 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["datasettr = get_tensor_dataset(df2, tokenizer)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["model_name=\"derbertav3_base_oversampling\""]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = 'true'"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["============================================================\n"," 4 -- 1e-05 -- 2022-09-20 15:57:21.055736\n","============================================================\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4916/4916 [00:02<00:00, 1825.26it/s]\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1229/1229 [00:00<00:00, 1794.99it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Train epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:12<00:00,  2.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/7], Train Loss: 0.5928, Train Accuracy: 0.6837 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:10<00:00,  5.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.98      0.49      0.65      1072\n","           1       0.21      0.92      0.34       157\n","\n","    accuracy                           0.54      1229\n","   macro avg       0.59      0.70      0.49      1229\n","weighted avg       0.88      0.54      0.61      1229\n","\n","Epoch [1/7], Test Loss: 0.3347,Test Precision: 0.2072, Test Recall: 0.9172, Test Macro F1: 0.4934, Test Accuracy: 0.5411 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1072\n","           1       0.54      0.54      0.54       157\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.74      0.73      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5419, Test Recall: 0.5350, Test Macro F1: 0.7357, Test Accuracy: 0.8828 \n","Test Precision: 0.6289, Test Recall: 0.3885, Test Macro F1: 0.7102, Test Accuracy: 0.8926 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:21<00:00,  2.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/7], Train Loss: 0.4645, Train Accuracy: 0.8059 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:11<00:00,  4.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.65      0.78      1072\n","           1       0.26      0.83      0.40       157\n","\n","    accuracy                           0.67      1229\n","   macro avg       0.61      0.74      0.59      1229\n","weighted avg       0.87      0.67      0.73      1229\n","\n","Epoch [2/7], Test Loss: 0.3444,Test Precision: 0.2594, Test Recall: 0.8344, Test Macro F1: 0.5865, Test Accuracy: 0.6745 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.91      0.92      1072\n","           1       0.48      0.58      0.53       157\n","\n","    accuracy                           0.87      1229\n","   macro avg       0.71      0.74      0.72      1229\n","weighted avg       0.88      0.87      0.87      1229\n","\n","Test Precision: 0.4815, Test Recall: 0.5796, Test Macro F1: 0.7242, Test Accuracy: 0.8666 \n","Test Precision: 0.5584, Test Recall: 0.5478, Test Macro F1: 0.7442, Test Accuracy: 0.8869 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:23<00:00,  2.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/7], Train Loss: 0.3842, Train Accuracy: 0.8496 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:14<00:00,  3.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.81      0.87      1072\n","           1       0.35      0.71      0.47       157\n","\n","    accuracy                           0.79      1229\n","   macro avg       0.65      0.76      0.67      1229\n","weighted avg       0.87      0.79      0.82      1229\n","\n","Epoch [3/7], Test Loss: 0.3282,Test Precision: 0.3511, Test Recall: 0.7134, Test Macro F1: 0.6717, Test Accuracy: 0.7950 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1072\n","           1       0.53      0.54      0.53       157\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.73      0.74      0.73      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5280, Test Recall: 0.5414, Test Macro F1: 0.7327, Test Accuracy: 0.8796 \n","Test Precision: 0.5786, Test Recall: 0.5159, Test Macro F1: 0.7415, Test Accuracy: 0.8902 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:26<00:00,  2.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/7], Train Loss: 0.3729, Train Accuracy: 0.8564 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.83      1072\n","           1       0.29      0.73      0.42       157\n","\n","    accuracy                           0.74      1229\n","   macro avg       0.62      0.74      0.62      1229\n","weighted avg       0.87      0.74      0.78      1229\n","\n","Epoch [4/7], Test Loss: 0.3427,Test Precision: 0.2911, Test Recall: 0.7325, Test Macro F1: 0.6239, Test Accuracy: 0.7380 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1072\n","           1       0.52      0.56      0.54       157\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.73      0.74      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5238, Test Recall: 0.5605, Test Macro F1: 0.7358, Test Accuracy: 0.8788 \n","Test Precision: 0.5556, Test Recall: 0.5096, Test Macro F1: 0.7331, Test Accuracy: 0.8853 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:24<00:00,  2.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/7], Train Loss: 0.3488, Train Accuracy: 0.8663 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:15<00:00,  3.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.83      1072\n","           1       0.29      0.73      0.42       157\n","\n","    accuracy                           0.74      1229\n","   macro avg       0.62      0.74      0.62      1229\n","weighted avg       0.87      0.74      0.78      1229\n","\n","Epoch [5/7], Test Loss: 0.3427,Test Precision: 0.2911, Test Recall: 0.7325, Test Macro F1: 0.6239, Test Accuracy: 0.7380 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1072\n","           1       0.52      0.56      0.54       157\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.73      0.74      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5238, Test Recall: 0.5605, Test Macro F1: 0.7358, Test Accuracy: 0.8788 \n","Test Precision: 0.5556, Test Recall: 0.5096, Test Macro F1: 0.7331, Test Accuracy: 0.8853 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:21<00:00,  2.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/7], Train Loss: 0.3426, Train Accuracy: 0.8655 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:14<00:00,  3.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.83      1072\n","           1       0.29      0.73      0.42       157\n","\n","    accuracy                           0.74      1229\n","   macro avg       0.62      0.74      0.62      1229\n","weighted avg       0.87      0.74      0.78      1229\n","\n","Epoch [6/7], Test Loss: 0.3427,Test Precision: 0.2911, Test Recall: 0.7325, Test Macro F1: 0.6239, Test Accuracy: 0.7380 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1072\n","           1       0.52      0.56      0.54       157\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.73      0.74      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5238, Test Recall: 0.5605, Test Macro F1: 0.7358, Test Accuracy: 0.8788 \n","Test Precision: 0.5556, Test Recall: 0.5096, Test Macro F1: 0.7331, Test Accuracy: 0.8853 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:27<00:00,  2.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/7], Train Loss: 0.3505, Train Accuracy: 0.8621 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:12<00:00,  4.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.83      1072\n","           1       0.29      0.73      0.42       157\n","\n","    accuracy                           0.74      1229\n","   macro avg       0.62      0.74      0.62      1229\n","weighted avg       0.87      0.74      0.78      1229\n","\n","Epoch [7/7], Test Loss: 0.3427,Test Precision: 0.2911, Test Recall: 0.7325, Test Macro F1: 0.6239, Test Accuracy: 0.7380 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1072\n","           1       0.52      0.56      0.54       157\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.73      0.74      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5238, Test Recall: 0.5605, Test Macro F1: 0.7358, Test Accuracy: 0.8788 \n","Test Precision: 0.5556, Test Recall: 0.5096, Test Macro F1: 0.7331, Test Accuracy: 0.8853 \n","Training complete!\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4916/4916 [00:02<00:00, 1872.51it/s]\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1229/1229 [00:00<00:00, 1782.63it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Train epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:21<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/7], Train Loss: 0.5928, Train Accuracy: 0.6913 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:15<00:00,  3.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.77      0.85      1072\n","           1       0.29      0.65      0.40       157\n","\n","    accuracy                           0.76      1229\n","   macro avg       0.62      0.71      0.63      1229\n","weighted avg       0.86      0.76      0.79      1229\n","\n","Epoch [1/7], Test Loss: 0.3327,Test Precision: 0.2939, Test Recall: 0.6497, Test Macro F1: 0.6256, Test Accuracy: 0.7559 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1072\n","           1       0.52      0.49      0.50       157\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.72      0.71      0.72      1229\n","weighted avg       0.87      0.88      0.87      1229\n","\n","Test Precision: 0.5168, Test Recall: 0.4904, Test Macro F1: 0.7163, Test Accuracy: 0.8763 \n","Test Precision: 1.0000, Test Recall: 0.0000, Test Macro F1: 0.4659, Test Accuracy: 0.8723 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:32<00:00,  2.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/7], Train Loss: 0.4566, Train Accuracy: 0.8078 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.66      0.78      1072\n","           1       0.24      0.74      0.36       157\n","\n","    accuracy                           0.67      1229\n","   macro avg       0.59      0.70      0.57      1229\n","weighted avg       0.85      0.67      0.72      1229\n","\n","Epoch [2/7], Test Loss: 0.3785,Test Precision: 0.2402, Test Recall: 0.7389, Test Macro F1: 0.5690, Test Accuracy: 0.6680 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.91      0.92      1072\n","           1       0.48      0.54      0.51       157\n","\n","    accuracy                           0.87      1229\n","   macro avg       0.71      0.73      0.72      1229\n","weighted avg       0.87      0.87      0.87      1229\n","\n","Test Precision: 0.4802, Test Recall: 0.5414, Test Macro F1: 0.7159, Test Accuracy: 0.8666 \n","Test Precision: 0.5241, Test Recall: 0.4841, Test Macro F1: 0.7169, Test Accuracy: 0.8779 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:29<00:00,  2.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/7], Train Loss: 0.3878, Train Accuracy: 0.8468 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.66      0.78      1072\n","           1       0.25      0.75      0.37       157\n","\n","    accuracy                           0.67      1229\n","   macro avg       0.60      0.71      0.58      1229\n","weighted avg       0.86      0.67      0.73      1229\n","\n","Epoch [3/7], Test Loss: 0.4318,Test Precision: 0.2463, Test Recall: 0.7516, Test Macro F1: 0.5758, Test Accuracy: 0.6745 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.88      0.91      1072\n","           1       0.42      0.58      0.49       157\n","\n","    accuracy                           0.85      1229\n","   macro avg       0.68      0.73      0.70      1229\n","weighted avg       0.87      0.85      0.86      1229\n","\n","Test Precision: 0.4233, Test Recall: 0.5796, Test Macro F1: 0.6991, Test Accuracy: 0.8454 \n","Test Precision: 0.4889, Test Recall: 0.5605, Test Macro F1: 0.7232, Test Accuracy: 0.8690 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:26<00:00,  2.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/7], Train Loss: 0.3532, Train Accuracy: 0.8628 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.71      0.81      1072\n","           1       0.27      0.71      0.39       157\n","\n","    accuracy                           0.71      1229\n","   macro avg       0.60      0.71      0.60      1229\n","weighted avg       0.86      0.71      0.76      1229\n","\n","Epoch [4/7], Test Loss: 0.3953,Test Precision: 0.2656, Test Recall: 0.7070, Test Macro F1: 0.5993, Test Accuracy: 0.7128 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.91      0.92      1072\n","           1       0.48      0.55      0.52       157\n","\n","    accuracy                           0.87      1229\n","   macro avg       0.71      0.73      0.72      1229\n","weighted avg       0.88      0.87      0.87      1229\n","\n","Test Precision: 0.4833, Test Recall: 0.5541, Test Macro F1: 0.7197, Test Accuracy: 0.8674 \n","Test Precision: 0.5238, Test Recall: 0.4904, Test Macro F1: 0.7185, Test Accuracy: 0.8779 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:22<00:00,  2.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/7], Train Loss: 0.3287, Train Accuracy: 0.8762 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.71      0.81      1072\n","           1       0.27      0.71      0.39       157\n","\n","    accuracy                           0.71      1229\n","   macro avg       0.60      0.71      0.60      1229\n","weighted avg       0.86      0.71      0.76      1229\n","\n","Epoch [5/7], Test Loss: 0.3953,Test Precision: 0.2656, Test Recall: 0.7070, Test Macro F1: 0.5993, Test Accuracy: 0.7128 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.91      0.92      1072\n","           1       0.48      0.55      0.52       157\n","\n","    accuracy                           0.87      1229\n","   macro avg       0.71      0.73      0.72      1229\n","weighted avg       0.88      0.87      0.87      1229\n","\n","Test Precision: 0.4833, Test Recall: 0.5541, Test Macro F1: 0.7197, Test Accuracy: 0.8674 \n","Test Precision: 0.5238, Test Recall: 0.4904, Test Macro F1: 0.7185, Test Accuracy: 0.8779 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:29<00:00,  2.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/7], Train Loss: 0.3384, Train Accuracy: 0.8700 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.71      0.81      1072\n","           1       0.27      0.71      0.39       157\n","\n","    accuracy                           0.71      1229\n","   macro avg       0.60      0.71      0.60      1229\n","weighted avg       0.86      0.71      0.76      1229\n","\n","Epoch [6/7], Test Loss: 0.3953,Test Precision: 0.2656, Test Recall: 0.7070, Test Macro F1: 0.5993, Test Accuracy: 0.7128 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.91      0.92      1072\n","           1       0.48      0.55      0.52       157\n","\n","    accuracy                           0.87      1229\n","   macro avg       0.71      0.73      0.72      1229\n","weighted avg       0.88      0.87      0.87      1229\n","\n","Test Precision: 0.4833, Test Recall: 0.5541, Test Macro F1: 0.7197, Test Accuracy: 0.8674 \n","Test Precision: 0.5238, Test Recall: 0.4904, Test Macro F1: 0.7185, Test Accuracy: 0.8779 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:30<00:00,  2.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/7], Train Loss: 0.3611, Train Accuracy: 0.8621 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      0.71      0.81      1072\n","           1       0.27      0.71      0.39       157\n","\n","    accuracy                           0.71      1229\n","   macro avg       0.60      0.71      0.60      1229\n","weighted avg       0.86      0.71      0.76      1229\n","\n","Epoch [7/7], Test Loss: 0.3953,Test Precision: 0.2656, Test Recall: 0.7070, Test Macro F1: 0.5993, Test Accuracy: 0.7128 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.91      0.92      1072\n","           1       0.48      0.55      0.52       157\n","\n","    accuracy                           0.87      1229\n","   macro avg       0.71      0.73      0.72      1229\n","weighted avg       0.88      0.87      0.87      1229\n","\n","Test Precision: 0.4833, Test Recall: 0.5541, Test Macro F1: 0.7197, Test Accuracy: 0.8674 \n","Test Precision: 0.5238, Test Recall: 0.4904, Test Macro F1: 0.7185, Test Accuracy: 0.8779 \n","Training complete!\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4916/4916 [00:02<00:00, 1684.13it/s]\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1229/1229 [00:00<00:00, 1578.64it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Train epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:24<00:00,  2.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/7], Train Loss: 0.5959, Train Accuracy: 0.6779 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:15<00:00,  3.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.58      0.72      1071\n","           1       0.23      0.84      0.35       158\n","\n","    accuracy                           0.61      1229\n","   macro avg       0.59      0.71      0.54      1229\n","weighted avg       0.87      0.61      0.67      1229\n","\n","Epoch [1/7], Test Loss: 0.3480,Test Precision: 0.2253, Test Recall: 0.8354, Test Macro F1: 0.5374, Test Accuracy: 0.6094 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.88      0.91      1071\n","           1       0.43      0.60      0.50       158\n","\n","    accuracy                           0.85      1229\n","   macro avg       0.68      0.74      0.71      1229\n","weighted avg       0.87      0.85      0.86      1229\n","\n","Test Precision: 0.4318, Test Recall: 0.6013, Test Macro F1: 0.7061, Test Accuracy: 0.8470 \n","Test Precision: 1.0000, Test Recall: 0.0000, Test Macro F1: 0.4657, Test Accuracy: 0.8714 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:29<00:00,  2.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/7], Train Loss: 0.4188, Train Accuracy: 0.8322 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:11<00:00,  4.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.76      0.85      1071\n","           1       0.31      0.73      0.44       158\n","\n","    accuracy                           0.76      1229\n","   macro avg       0.63      0.75      0.64      1229\n","weighted avg       0.87      0.76      0.79      1229\n","\n","Epoch [2/7], Test Loss: 0.3430,Test Precision: 0.3144, Test Recall: 0.7342, Test Macro F1: 0.6437, Test Accuracy: 0.7600 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.92      0.93      1071\n","           1       0.52      0.61      0.56       158\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.73      0.76      0.74      1229\n","weighted avg       0.89      0.88      0.88      1229\n","\n","Test Precision: 0.5189, Test Recall: 0.6076, Test Macro F1: 0.7442, Test Accuracy: 0.8771 \n","Test Precision: 0.5676, Test Recall: 0.5316, Test Macro F1: 0.7424, Test Accuracy: 0.8877 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:23<00:00,  2.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/7], Train Loss: 0.4069, Train Accuracy: 0.8284 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.63      0.76      1071\n","           1       0.24      0.78      0.37       158\n","\n","    accuracy                           0.65      1229\n","   macro avg       0.60      0.71      0.56      1229\n","weighted avg       0.86      0.65      0.71      1229\n","\n","Epoch [3/7], Test Loss: 0.4033,Test Precision: 0.2385, Test Recall: 0.7848, Test Macro F1: 0.5621, Test Accuracy: 0.6501 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.87      0.91      1071\n","           1       0.42      0.64      0.51       158\n","\n","    accuracy                           0.84      1229\n","   macro avg       0.68      0.76      0.71      1229\n","weighted avg       0.88      0.84      0.85      1229\n","\n","Test Precision: 0.4226, Test Recall: 0.6392, Test Macro F1: 0.7071, Test Accuracy: 0.8413 \n","Test Precision: 0.5112, Test Recall: 0.5759, Test Macro F1: 0.7345, Test Accuracy: 0.8747 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:28<00:00,  2.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/7], Train Loss: 0.3409, Train Accuracy: 0.8670 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:12<00:00,  4.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.84      1071\n","           1       0.30      0.75      0.43       158\n","\n","    accuracy                           0.75      1229\n","   macro avg       0.63      0.75      0.63      1229\n","weighted avg       0.87      0.75      0.78      1229\n","\n","Epoch [4/7], Test Loss: 0.3730,Test Precision: 0.3028, Test Recall: 0.7532, Test Macro F1: 0.6339, Test Accuracy: 0.7453 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.89      0.92      1071\n","           1       0.47      0.65      0.55       158\n","\n","    accuracy                           0.86      1229\n","   macro avg       0.71      0.77      0.73      1229\n","weighted avg       0.88      0.86      0.87      1229\n","\n","Test Precision: 0.4722, Test Recall: 0.6456, Test Macro F1: 0.7319, Test Accuracy: 0.8617 \n","Test Precision: 0.5263, Test Recall: 0.5696, Test Macro F1: 0.7386, Test Accuracy: 0.8788 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:24<00:00,  2.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/7], Train Loss: 0.3335, Train Accuracy: 0.8680 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:14<00:00,  3.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.84      1071\n","           1       0.30      0.75      0.43       158\n","\n","    accuracy                           0.75      1229\n","   macro avg       0.63      0.75      0.63      1229\n","weighted avg       0.87      0.75      0.78      1229\n","\n","Epoch [5/7], Test Loss: 0.3730,Test Precision: 0.3028, Test Recall: 0.7532, Test Macro F1: 0.6339, Test Accuracy: 0.7453 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.89      0.92      1071\n","           1       0.47      0.65      0.55       158\n","\n","    accuracy                           0.86      1229\n","   macro avg       0.71      0.77      0.73      1229\n","weighted avg       0.88      0.86      0.87      1229\n","\n","Test Precision: 0.4722, Test Recall: 0.6456, Test Macro F1: 0.7319, Test Accuracy: 0.8617 \n","Test Precision: 0.5263, Test Recall: 0.5696, Test Macro F1: 0.7386, Test Accuracy: 0.8788 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:31<00:00,  2.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/7], Train Loss: 0.3277, Train Accuracy: 0.8749 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:12<00:00,  4.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.84      1071\n","           1       0.30      0.75      0.43       158\n","\n","    accuracy                           0.75      1229\n","   macro avg       0.63      0.75      0.63      1229\n","weighted avg       0.87      0.75      0.78      1229\n","\n","Epoch [6/7], Test Loss: 0.3730,Test Precision: 0.3028, Test Recall: 0.7532, Test Macro F1: 0.6339, Test Accuracy: 0.7453 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.89      0.92      1071\n","           1       0.47      0.65      0.55       158\n","\n","    accuracy                           0.86      1229\n","   macro avg       0.71      0.77      0.73      1229\n","weighted avg       0.88      0.86      0.87      1229\n","\n","Test Precision: 0.4722, Test Recall: 0.6456, Test Macro F1: 0.7319, Test Accuracy: 0.8617 \n","Test Precision: 0.5263, Test Recall: 0.5696, Test Macro F1: 0.7386, Test Accuracy: 0.8788 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:22<00:00,  2.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/7], Train Loss: 0.3347, Train Accuracy: 0.8730 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:12<00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.84      1071\n","           1       0.30      0.75      0.43       158\n","\n","    accuracy                           0.75      1229\n","   macro avg       0.63      0.75      0.63      1229\n","weighted avg       0.87      0.75      0.78      1229\n","\n","Epoch [7/7], Test Loss: 0.3730,Test Precision: 0.3028, Test Recall: 0.7532, Test Macro F1: 0.6339, Test Accuracy: 0.7453 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.89      0.92      1071\n","           1       0.47      0.65      0.55       158\n","\n","    accuracy                           0.86      1229\n","   macro avg       0.71      0.77      0.73      1229\n","weighted avg       0.88      0.86      0.87      1229\n","\n","Test Precision: 0.4722, Test Recall: 0.6456, Test Macro F1: 0.7319, Test Accuracy: 0.8617 \n","Test Precision: 0.5263, Test Recall: 0.5696, Test Macro F1: 0.7386, Test Accuracy: 0.8788 \n","Training complete!\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4916/4916 [00:02<00:00, 1752.86it/s]\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1229/1229 [00:00<00:00, 1962.14it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Train epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:22<00:00,  2.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/7], Train Loss: 0.6004, Train Accuracy: 0.6778 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:14<00:00,  3.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.72      0.82      1071\n","           1       0.28      0.75      0.41       158\n","\n","    accuracy                           0.72      1229\n","   macro avg       0.62      0.73      0.61      1229\n","weighted avg       0.87      0.72      0.76      1229\n","\n","Epoch [1/7], Test Loss: 0.3178,Test Precision: 0.2813, Test Recall: 0.7532, Test Macro F1: 0.6135, Test Accuracy: 0.7209 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.94      0.93      1071\n","           1       0.55      0.53      0.54       158\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.74      0.73      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5490, Test Recall: 0.5316, Test Macro F1: 0.7368, Test Accuracy: 0.8836 \n","Test Precision: 1.0000, Test Recall: 0.0000, Test Macro F1: 0.4657, Test Accuracy: 0.8714 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:23<00:00,  2.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/7], Train Loss: 0.4647, Train Accuracy: 0.8021 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:14<00:00,  3.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.68      0.80      1071\n","           1       0.27      0.78      0.40       158\n","\n","    accuracy                           0.70      1229\n","   macro avg       0.61      0.73      0.60      1229\n","weighted avg       0.87      0.70      0.75      1229\n","\n","Epoch [2/7], Test Loss: 0.3191,Test Precision: 0.2672, Test Recall: 0.7848, Test Macro F1: 0.5975, Test Accuracy: 0.6957 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.95      0.94      1071\n","           1       0.58      0.50      0.54       158\n","\n","    accuracy                           0.89      1229\n","   macro avg       0.75      0.72      0.74      1229\n","weighted avg       0.88      0.89      0.89      1229\n","\n","Test Precision: 0.5766, Test Recall: 0.5000, Test Macro F1: 0.7361, Test Accuracy: 0.8885 \n","Test Precision: 0.6569, Test Recall: 0.4241, Test Macro F1: 0.7290, Test Accuracy: 0.8975 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:26<00:00,  2.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/7], Train Loss: 0.3989, Train Accuracy: 0.8383 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.64      0.77      1071\n","           1       0.24      0.78      0.37       158\n","\n","    accuracy                           0.66      1229\n","   macro avg       0.60      0.71      0.57      1229\n","weighted avg       0.86      0.66      0.72      1229\n","\n","Epoch [3/7], Test Loss: 0.3592,Test Precision: 0.2441, Test Recall: 0.7848, Test Macro F1: 0.5696, Test Accuracy: 0.6599 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.91      0.92      1071\n","           1       0.48      0.54      0.51       158\n","\n","    accuracy                           0.87      1229\n","   macro avg       0.71      0.73      0.72      1229\n","weighted avg       0.87      0.87      0.87      1229\n","\n","Test Precision: 0.4831, Test Recall: 0.5443, Test Macro F1: 0.7173, Test Accuracy: 0.8666 \n","Test Precision: 0.5833, Test Recall: 0.4873, Test Macro F1: 0.7342, Test Accuracy: 0.8893 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:23<00:00,  2.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/7], Train Loss: 0.3424, Train Accuracy: 0.8636 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:12<00:00,  4.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.83      1071\n","           1       0.29      0.74      0.42       158\n","\n","    accuracy                           0.74      1229\n","   macro avg       0.62      0.74      0.63      1229\n","weighted avg       0.87      0.74      0.78      1229\n","\n","Epoch [4/7], Test Loss: 0.3400,Test Precision: 0.2932, Test Recall: 0.7405, Test Macro F1: 0.6251, Test Accuracy: 0.7372 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1071\n","           1       0.54      0.55      0.55       158\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.74      0.74      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5404, Test Recall: 0.5506, Test Macro F1: 0.7388, Test Accuracy: 0.8820 \n","Test Precision: 0.6148, Test Recall: 0.4747, Test Macro F1: 0.7380, Test Accuracy: 0.8942 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:27<00:00,  2.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/7], Train Loss: 0.3409, Train Accuracy: 0.8648 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.83      1071\n","           1       0.29      0.74      0.42       158\n","\n","    accuracy                           0.74      1229\n","   macro avg       0.62      0.74      0.63      1229\n","weighted avg       0.87      0.74      0.78      1229\n","\n","Epoch [5/7], Test Loss: 0.3400,Test Precision: 0.2932, Test Recall: 0.7405, Test Macro F1: 0.6251, Test Accuracy: 0.7372 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1071\n","           1       0.54      0.55      0.55       158\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.74      0.74      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5404, Test Recall: 0.5506, Test Macro F1: 0.7388, Test Accuracy: 0.8820 \n","Test Precision: 0.6148, Test Recall: 0.4747, Test Macro F1: 0.7380, Test Accuracy: 0.8942 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:31<00:00,  2.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/7], Train Loss: 0.3256, Train Accuracy: 0.8757 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.83      1071\n","           1       0.29      0.74      0.42       158\n","\n","    accuracy                           0.74      1229\n","   macro avg       0.62      0.74      0.63      1229\n","weighted avg       0.87      0.74      0.78      1229\n","\n","Epoch [6/7], Test Loss: 0.3400,Test Precision: 0.2932, Test Recall: 0.7405, Test Macro F1: 0.6251, Test Accuracy: 0.7372 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1071\n","           1       0.54      0.55      0.55       158\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.74      0.74      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5404, Test Recall: 0.5506, Test Macro F1: 0.7388, Test Accuracy: 0.8820 \n","Test Precision: 0.6148, Test Recall: 0.4747, Test Macro F1: 0.7380, Test Accuracy: 0.8942 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:25<00:00,  2.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/7], Train Loss: 0.3411, Train Accuracy: 0.8639 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:14<00:00,  3.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.74      0.83      1071\n","           1       0.29      0.74      0.42       158\n","\n","    accuracy                           0.74      1229\n","   macro avg       0.62      0.74      0.63      1229\n","weighted avg       0.87      0.74      0.78      1229\n","\n","Epoch [7/7], Test Loss: 0.3400,Test Precision: 0.2932, Test Recall: 0.7405, Test Macro F1: 0.6251, Test Accuracy: 0.7372 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1071\n","           1       0.54      0.55      0.55       158\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.74      0.74      0.74      1229\n","weighted avg       0.88      0.88      0.88      1229\n","\n","Test Precision: 0.5404, Test Recall: 0.5506, Test Macro F1: 0.7388, Test Accuracy: 0.8820 \n","Test Precision: 0.6148, Test Recall: 0.4747, Test Macro F1: 0.7380, Test Accuracy: 0.8942 \n","Training complete!\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4916/4916 [00:02<00:00, 1729.75it/s]\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1229/1229 [00:00<00:00, 2091.34it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Train epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:28<00:00,  2.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/7], Train Loss: 0.6029, Train Accuracy: 0.6801 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:12<00:00,  4.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.30      0.46      1071\n","           1       0.16      0.92      0.28       158\n","\n","    accuracy                           0.38      1229\n","   macro avg       0.56      0.61      0.37      1229\n","weighted avg       0.86      0.38      0.44      1229\n","\n","Epoch [1/7], Test Loss: 0.3452,Test Precision: 0.1629, Test Recall: 0.9177, Test Macro F1: 0.3696, Test Accuracy: 0.3832 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1071\n","           1       0.55      0.56      0.55       158\n","\n","    accuracy                           0.88      1229\n","   macro avg       0.74      0.74      0.74      1229\n","weighted avg       0.89      0.88      0.88      1229\n","\n","Test Precision: 0.5500, Test Recall: 0.5570, Test Macro F1: 0.7436, Test Accuracy: 0.8845 \n","Test Precision: 1.0000, Test Recall: 0.0000, Test Macro F1: 0.4657, Test Accuracy: 0.8714 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:20<00:00,  2.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/7], Train Loss: 0.4484, Train Accuracy: 0.8164 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:15<00:00,  3.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.64      0.76      1071\n","           1       0.24      0.80      0.37       158\n","\n","    accuracy                           0.66      1229\n","   macro avg       0.60      0.72      0.57      1229\n","weighted avg       0.86      0.66      0.71      1229\n","\n","Epoch [2/7], Test Loss: 0.3304,Test Precision: 0.2442, Test Recall: 0.7975, Test Macro F1: 0.5687, Test Accuracy: 0.6566 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.93      0.93      1071\n","           1       0.56      0.57      0.56       158\n","\n","    accuracy                           0.89      1229\n","   macro avg       0.75      0.75      0.75      1229\n","weighted avg       0.89      0.89      0.89      1229\n","\n","Test Precision: 0.5556, Test Recall: 0.5696, Test Macro F1: 0.7485, Test Accuracy: 0.8861 \n","Test Precision: 0.6311, Test Recall: 0.4873, Test Macro F1: 0.7461, Test Accuracy: 0.8975 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:25<00:00,  2.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/7], Train Loss: 0.4025, Train Accuracy: 0.8333 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:15<00:00,  3.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.81      0.88      1071\n","           1       0.36      0.70      0.47       158\n","\n","    accuracy                           0.80      1229\n","   macro avg       0.65      0.76      0.67      1229\n","weighted avg       0.87      0.80      0.82      1229\n","\n","Epoch [3/7], Test Loss: 0.3067,Test Precision: 0.3558, Test Recall: 0.7025, Test Macro F1: 0.6738, Test Accuracy: 0.7982 \n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.94      0.94      1071\n","           1       0.58      0.58      0.58       158\n","\n","    accuracy                           0.89      1229\n","   macro avg       0.76      0.76      0.76      1229\n","weighted avg       0.89      0.89      0.89      1229\n","\n","Test Precision: 0.5759, Test Recall: 0.5759, Test Macro F1: 0.7567, Test Accuracy: 0.8910 \n","Test Precision: 0.6087, Test Recall: 0.5316, Test Macro F1: 0.7542, Test Accuracy: 0.8959 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:29<00:00,  2.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/7], Train Loss: 0.3537, Train Accuracy: 0.8600 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.79      0.86      1071\n","           1       0.32      0.70      0.44       158\n","\n","    accuracy                           0.77      1229\n","   macro avg       0.63      0.74      0.65      1229\n","weighted avg       0.87      0.77      0.80      1229\n","\n","Epoch [4/7], Test Loss: 0.3056,Test Precision: 0.3235, Test Recall: 0.6962, Test Macro F1: 0.6500, Test Accuracy: 0.7738 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.94      0.94      1071\n","           1       0.58      0.54      0.56       158\n","\n","    accuracy                           0.89      1229\n","   macro avg       0.76      0.74      0.75      1229\n","weighted avg       0.89      0.89      0.89      1229\n","\n","Test Precision: 0.5811, Test Recall: 0.5443, Test Macro F1: 0.7499, Test Accuracy: 0.8910 \n","Test Precision: 0.6638, Test Recall: 0.4873, Test Macro F1: 0.7535, Test Accuracy: 0.9024 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:30<00:00,  2.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/7], Train Loss: 0.3602, Train Accuracy: 0.8594 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:12<00:00,  4.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.79      0.86      1071\n","           1       0.32      0.70      0.44       158\n","\n","    accuracy                           0.77      1229\n","   macro avg       0.63      0.74      0.65      1229\n","weighted avg       0.87      0.77      0.80      1229\n","\n","Epoch [5/7], Test Loss: 0.3056,Test Precision: 0.3235, Test Recall: 0.6962, Test Macro F1: 0.6500, Test Accuracy: 0.7738 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.94      0.94      1071\n","           1       0.58      0.54      0.56       158\n","\n","    accuracy                           0.89      1229\n","   macro avg       0.76      0.74      0.75      1229\n","weighted avg       0.89      0.89      0.89      1229\n","\n","Test Precision: 0.5811, Test Recall: 0.5443, Test Macro F1: 0.7499, Test Accuracy: 0.8910 \n","Test Precision: 0.6638, Test Recall: 0.4873, Test Macro F1: 0.7535, Test Accuracy: 0.9024 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:28<00:00,  2.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/7], Train Loss: 0.3635, Train Accuracy: 0.8587 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:12<00:00,  4.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.79      0.86      1071\n","           1       0.32      0.70      0.44       158\n","\n","    accuracy                           0.77      1229\n","   macro avg       0.63      0.74      0.65      1229\n","weighted avg       0.87      0.77      0.80      1229\n","\n","Epoch [6/7], Test Loss: 0.3056,Test Precision: 0.3235, Test Recall: 0.6962, Test Macro F1: 0.6500, Test Accuracy: 0.7738 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.94      0.94      1071\n","           1       0.58      0.54      0.56       158\n","\n","    accuracy                           0.89      1229\n","   macro avg       0.76      0.74      0.75      1229\n","weighted avg       0.89      0.89      0.89      1229\n","\n","Test Precision: 0.5811, Test Recall: 0.5443, Test Macro F1: 0.7499, Test Accuracy: 0.8910 \n","Test Precision: 0.6638, Test Recall: 0.4873, Test Macro F1: 0.7535, Test Accuracy: 0.9024 \n"]},{"name":"stderr","output_type":"stream","text":["Train epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [01:26<00:00,  2.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/7], Train Loss: 0.3440, Train Accuracy: 0.8665 \n"]},{"name":"stderr","output_type":"stream","text":["Eval model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:13<00:00,  3.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.95      0.79      0.86      1071\n","           1       0.32      0.70      0.44       158\n","\n","    accuracy                           0.77      1229\n","   macro avg       0.63      0.74      0.65      1229\n","weighted avg       0.87      0.77      0.80      1229\n","\n","Epoch [7/7], Test Loss: 0.3056,Test Precision: 0.3235, Test Recall: 0.6962, Test Macro F1: 0.6500, Test Accuracy: 0.7738 \n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.94      0.94      1071\n","           1       0.58      0.54      0.56       158\n","\n","    accuracy                           0.89      1229\n","   macro avg       0.76      0.74      0.75      1229\n","weighted avg       0.89      0.89      0.89      1229\n","\n","Test Precision: 0.5811, Test Recall: 0.5443, Test Macro F1: 0.7499, Test Accuracy: 0.8910 \n","Test Precision: 0.6638, Test Recall: 0.4873, Test Macro F1: 0.7535, Test Accuracy: 0.9024 \n","Training complete!\n"]}],"source":["print(\"============================================================\")\n","print(f\" {epoch} -- {lr} -- {datetime.datetime.now()}\")\n","print(\"============================================================\")\n","\n","# with open(f'{model_name}_train.pkl', 'rb') as f:\n","#     train_dataset = pickle.load(f)\n","skf = StratifiedKFold(FOLDS, random_state=42, shuffle=True)\n","for fold, (tr_id, val_id) in enumerate(skf.split(df, df[\"IsInstructionClear\"])):\n","    # dfdev.to_csv('public_data/clarifying_questions_val_w_context.csv', index=False)\n","    dftrain = df.iloc[tr_id].copy()\n","    dfdev = df.iloc[val_id].copy()\n","\n","    dftrain[\"IsInstructionClear\"] = dftrain.IsInstructionClear.replace(\n","        {\"Yes\": 0, \"No\": 1}\n","    )\n","    oversampling_weights= dftrain[\"IsInstructionClear\"].apply(lambda x: 4 if x==1 else 1).values\n","\n","    dfdev[\"IsInstructionClear\"] = dfdev.IsInstructionClear.replace({\"Yes\": 0, \"No\": 1})\n","    from transformers import AutoTokenizer\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_hug)\n","\n","    datasettr = get_tensor_dataset(dftrain, tokenizer)\n","    datasetval = get_tensor_dataset(dfdev, tokenizer)\n","\n","    train_dataloader = DataLoader(\n","        datasettr,\n","        sampler=WeightedRandomSampler(oversampling_weights,num_samples=len(datasettr)),#RandomSampler(datasettr),\n","        batch_size=batch_size,\n","        pin_memory=True,\n","        num_workers=4,\n","    )\n","    val_dataloader = DataLoader(\n","        datasetval, batch_size=batch_size, pin_memory=True, num_workers=4\n","    )\n","\n","    from transformers import AutoModelForSequenceClassification\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(model_hug, num_labels=2)\n","    model.cuda()\n","\n","    optimizer = AdamW(\n","        model.parameters(), lr=lr, weight_decay=0.05  # 0.03 73.4 ar epoch 6\n","    )\n","\n","    criterion = CrossEntropyLoss()\n","    total_steps = len(train_dataloader) * epoch\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n","    )\n","    \n","    # Train Model\n","    train_model(\n","        model_name,\n","        model,\n","        train_dataloader,\n","        val_dataloader,\n","        scheduler,\n","        optimizer,\n","        criterion,\n","        epochs,\n","        lr,\n","        fold,\n","    )\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/plain":["'saved_model/derbertav3_base_oversampling_5/7e_1e-05lr_'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["f\"saved_model/{model_name}_5/7e_{lr}lr_\""]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["epochs=7"]},{"cell_type":"code","execution_count":162,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","/home/felipe/anaconda3/envs/iglu2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Tokenizing data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 683/683 [00:00<00:00, 1439.61it/s]\n"]}],"source":["\n","df_test[\"IsInstructionClear\"] = df_test.IsInstructionClear.replace({\"Yes\": 0, \"No\": 1})\n","from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_hug)\n","\n","\n","datasetval = get_tensor_dataset(df_test, tokenizer)\n","\n","\n","val_dataloader = DataLoader(\n","    datasetval, batch_size=batch_size, pin_memory=True, num_workers=4\n",")"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["device ='cuda'"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"data":{"text/plain":["'saved_model/deberta_v3_base_proper_5/7e_1e-05lr_f4/pytorch_model.bin'"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["f\"saved_model/{model_name}_{epoch}/{epochs}e_{lr}lr_f{fold}/pytorch_model.bin\""]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:06<00:00,  4.83it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:06<00:00,  4.81it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:05<00:00,  4.84it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:06<00:00,  4.78it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:06<00:00,  4.78it/s]\n"]},{"data":{"text/plain":["0.7495075027244531"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["all_res=[]\n","for fold in range(FOLDS):\n","  res=[]\n","  true_labels=[]\n","  model = AutoModelForSequenceClassification.from_pretrained(model_hug, num_labels=2)\n","  model.cuda()\n","  model.load_state_dict(torch.load(f\"saved_model/{model_name}_{epoch}/{epochs}e_{lr}lr_f{fold}/pytorch_model.bin\"))\n","  model.eval() \n","  \n","  for _, batch in tqdm(\n","            enumerate(val_dataloader),\n","            total=len(val_dataloader),\n","            #desc=f\"Train epoch {epoch+1}/{epochs}\",\n","        ):\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    with torch.no_grad():\n","      pred = torch.softmax(model(b_input_ids,attention_mask=b_input_mask).logits,axis=1).detach().cpu().numpy()\n","\n","    #res = int(pred[:,1]>0.5)\n","    res.append((pred[:,1]>0.5).astype(int))\n","    true_labels.append(b_labels.detach().cpu().numpy())\n","  all_res.append(np.concatenate(res))\n","all_res = np.stack(all_res,axis=1)\n","final = np.mean(all_res,axis=1)>3/5\n","labels=np.concatenate(true_labels)\n","\n","\n","f1_score(labels,final,average=\"macro\")"]},{"cell_type":"code","execution_count":247,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:05<00:00,  5.14it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:05<00:00,  5.03it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:05<00:00,  4.94it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:05<00:00,  4.86it/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n","- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:06<00:00,  4.79it/s]\n"]},{"data":{"text/plain":["0.7554600787683494"]},"execution_count":247,"metadata":{},"output_type":"execute_result"}],"source":["all_res=[]\n","for fold in range(FOLDS):\n","  res=[]\n","  true_labels=[]\n","  model = AutoModelForSequenceClassification.from_pretrained(model_hug, num_labels=2)\n","  model.cuda()\n","  model.load_state_dict(torch.load(f\"saved_model/{model_name}_{epoch}/{epochs}e_{lr}lr_f{fold}/pytorch_model.bin\"))\n","  model.eval() \n","  \n","  for _, batch in tqdm(\n","            enumerate(val_dataloader),\n","            total=len(val_dataloader),\n","            #desc=f\"Train epoch {epoch+1}/{epochs}\",\n","        ):\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    with torch.no_grad():\n","      pred = torch.softmax(model(b_input_ids,attention_mask=b_input_mask).logits,axis=1).detach().cpu().numpy()\n","\n","    #res = int(pred[:,1]>0.5)\n","    #res.append((pred[:,1]>0.5).astype(int))\n","    res.append((pred[:,1]))\n","    true_labels.append(b_labels.detach().cpu().numpy())\n","  all_res.append(np.concatenate(res))\n","all_res = np.stack(all_res,axis=1)\n","final = np.mean(all_res,axis=1)>0.5\n","labels=np.concatenate(true_labels)\n","\n","\n","f1_score(labels,final,average=\"macro\")"]},{"cell_type":"code","execution_count":245,"metadata":{},"outputs":[{"data":{"text/plain":["0.7671983950761561"]},"execution_count":245,"metadata":{},"output_type":"execute_result"}],"source":["final = np.mean(all_res,axis=1)>0.65\n","labels=np.concatenate(true_labels)\n","f1_score(labels,final,average=\"macro\")"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["block_colour_name_map = {\n","    # voxelworld's colour id : iglu colour id\n","    0: \"air\",\n","    1: \"blue\",\n","    6: \"yellow\",\n","    2: \"green\",\n","    4: \"orange\",\n","    5: \"purple\",\n","    3: \"red\",\n","}"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["\n","original_colours =[val for key,val in block_colour_name_map.items()if val !='air']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gen = default_rng(seed=42)\n","gen.permutation(original_colours)\n","cur_per = gen.permutation(original_colours)\n","no_per = {key:key for key in original_colours}\n","per_dict ={ key:val for key,val in zip(original_colours,cur_per)}"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["import re\n","def replace_full_words(text, dic):\n","    for i, j in dic.items():\n","        text = re.sub(r\"\\b%s\\b\" % i, j, text)\n","        # r\"\\b%s\\b\"% enables replacing by whole word matches only\n","    return text"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["sentence = 'There are 6 levels. There are 12 different blocks. At the 0th level there are 2 red  blocks  Above at the 1st level there are 4 red  blocks  Above at the 2nd level there are 2 red and 1 green  blocks  Above at the 3rd level there are 1 green  blocks  Above at the 4th level there are 1 green  blocks  Above at the 5th level there are 1 green  blocks '"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["'There are 6 levels. There are 12 different blocks. At the 0th level there are 2 red  blocks  Above at the 1st level there are 4 red  blocks  Above at the 2nd level there are 2 red and 1 blue  blocks  Above at the 3rd level there are 1 blue  blocks  Above at the 4th level there are 1 blue  blocks  Above at the 5th level there are 1 blue  blocks '"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["replace_full_words(sentence,per_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_tensor_dataset_with_permutations(df, tokenizer,per_dict=no_per):\n","\n","    # Tokenize all of the sentences and map the tokens to thier word IDs.\n","    input_ids = []\n","    token_type_ids = []\n","    attention_masks = []\n","    labels = []\n","    topic_ids = []\n","\n","    for count, item in tqdm(\n","        enumerate(\n","            zip(\n","                df[\"GameId\"],\n","                df[\"bylevel_color_context\"],  # df[\"nonspatial_color_context\"],\n","                df[\"InputInstruction\"],\n","                df[\"IsInstructionClear\"],\n","            )\n","        ),\n","        total=len(df),\n","        desc=\"Tokenizing data\",\n","    ):\n","        z, w, x, y = item\n","\n","        w = replace_full_words(w,per_dict)\n","        x = replace_full_words(x,per_dict)\n","        encoded_dict = tokenizer.encode_plus(\n","            w,\n","            x,\n","            add_special_tokens=True,\n","            max_length=max_seq_length,\n","            padding=\"max_length\",  # use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'`\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        input_ids.append(encoded_dict[\"input_ids\"])\n","\n","        if \"token_type_ids\" in encoded_dict:\n","            token_type_ids.append(encoded_dict[\"token_type_ids\"])\n","\n","        attention_masks.append(encoded_dict[\"attention_mask\"])\n","        labels.append(y)\n","\n","        topic_ids.append(z)\n","\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels)\n","    return TensorDataset(input_ids, attention_masks, labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_hug)\n","all_per_preds=[]\n","for step in range(5):\n","  if step==0:\n","      datasetval = get_tensor_dataset_with_permutations(df_test, tokenizer)\n","  else:\n","      cur_per = gen.permutation(original_colours)\n","      per_dict ={ key:val for key,val in zip(original_colours,cur_per)}\n","      datasetval = get_tensor_dataset_with_permutations(df_test, tokenizer,per_dict)\n","\n","  val_dataloader = DataLoader(\n","      datasetval, batch_size=batch_size, pin_memory=True, num_workers=4\n","  )\n","\n","  all_res=[]\n","  for fold in range(FOLDS):\n","    res=[]\n","    true_labels=[]\n","    model = AutoModelForSequenceClassification.from_pretrained(model_hug, num_labels=2)\n","    model.cuda()\n","    model.load_state_dict(torch.load(f\"saved_model/{model_name}_{epoch}/{epochs}e_{lr}lr_f{fold}/pytorch_model.bin\"))\n","    model.eval() \n","    \n","    for _, batch in tqdm(\n","              enumerate(val_dataloader),\n","              total=len(val_dataloader),\n","              #desc=f\"Train epoch {epoch+1}/{epochs}\",\n","          ):\n","      b_input_ids = batch[0].to(device)\n","      b_input_mask = batch[1].to(device)\n","      b_labels = batch[2].to(device)\n","      with torch.no_grad():\n","        pred = torch.softmax(model(b_input_ids,attention_mask=b_input_mask).logits,axis=1).detach().cpu().numpy()\n","\n","      #res = int(pred[:,1]>0.5)\n","      #res.append((pred[:,1]>0.5).astype(int))\n","      res.append((pred[:,1]))\n","      true_labels.append(b_labels.detach().cpu().numpy())\n","    all_res.append(np.concatenate(res))\n","  all_res = np.stack(all_res,axis=1)\n","  final = np.mean(all_res,axis=1)>0.5\n","  labels=np.concatenate(true_labels)\n","\n","\n","  f1_score(labels,final,average=\"macro\")\n","  del model\n","  del batch\n","  torch.cuda.empty_cache()\n","\n","  all_per_preds.append(all_res)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"IGLU 2022 - NLP  Baseline- BERT Classifier - BM25 Ranker.ipynb","provenance":[{"file_id":"1R2s4Y2yPQC4glDN72EqAEphirSJjajEX","timestamp":1659449289022},{"file_id":"1C7fBGcZXfKo4dv7pYhrXc51lt6VzxGLg","timestamp":1659399381027}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.4 ('iglu2')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"bce01a44bbe2ca620061f41984838f4961c7f78d841ffeab292f7643abbc5513"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"03a618ff4ee0401f957d85497747bfdd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03db55e7ae8b44a591634f9edc458624":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03fef63aa66a49309fb2436854d15d24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a872f5d3f3843478774e7bf3045be85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e401b29f14f247e1b649df1ba8779dd7","IPY_MODEL_ea7730e4d9ea4a7bbc1cf2572434549e","IPY_MODEL_e940e19e08154c09b9c789db00fee1f4"],"layout":"IPY_MODEL_1f10841e430c47988a0e9ee0775f5377"}},"0fe02fa28f11441fa2e56b8a50e5bb97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"152583b8434044029fc3dd8c3b01019e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"158c3bef95684e64a15dc20cbc0e6883":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17d2a8fdb0ce42089433a0619b8bc377":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1876f748227547aba4398b1e4b399199":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03fef63aa66a49309fb2436854d15d24","placeholder":"â€‹","style":"IPY_MODEL_ec0ac8e334864dde9c94ec8ae5289635","value":" 570/570 [00:00&lt;00:00, 13.3kB/s]"}},"1a599c3bee5b4d038c50f768119e8681":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a06be6d23355447698547fb272aa1fce","placeholder":"â€‹","style":"IPY_MODEL_d8b2c8a3b51849888f0f9d9abf553874","value":"Eval model: 100%"}},"1ca9ee7c77c545c58b7dfac7a0a33a5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eeb928899284259a2a28bdeb7ddc7e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f10841e430c47988a0e9ee0775f5377":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"217a147af5e4480bb42e7aaea92b0687":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"282675c3d54e4c7f97bf688a12eedd34":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b725e64b982402cbd179f0708a649d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89887da51a1d444dabb7ca8c95175ee0","placeholder":"â€‹","style":"IPY_MODEL_39c7368208674130b7fd31216d881668","value":" 420M/420M [00:25&lt;00:00, 18.1MB/s]"}},"2b8235324e9043849ae3ce48310a10e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bfe0f794fe144aa81187c4affe9f7ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c8eae5b06ec43118f99a850374175ec","placeholder":"â€‹","style":"IPY_MODEL_454945d525b8452e82c50882b1e409b6","value":" 28.0/28.0 [00:00&lt;00:00, 834B/s]"}},"2e2a37ded05a402890e9188bbfde6f03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc2c9640e77a4986ace4b4e00488f030","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f87ab37f48c4ea699466e0a244a3df8","value":363}},"330e9455dae8406ea6cf48463b4bd373":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"334e1386cf2a47e69bf4bb793a4829c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"334e3c7a448948e09ca448c5d7cdf442":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c4af82f506946cd880d337358516831","placeholder":"â€‹","style":"IPY_MODEL_334e1386cf2a47e69bf4bb793a4829c5","value":" 363/363 [02:05&lt;00:00,  3.10it/s]"}},"35fa9d72c9ac488892631ced49b6a08e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36582915552c44f4a18fc221b0ebd6a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36bc4af466ea486ab0b9e30da832007a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae445705931a41ec80ed1047672628f5","placeholder":"â€‹","style":"IPY_MODEL_e708cadd2e4a405e8adcb297182ac88a","value":" 60.3k/60.3k [00:00&lt;00:00, 355kB/s]"}},"38a3616001514d409e8589fa51545fee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39c7368208674130b7fd31216d881668":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b22025d2e034bf290207b9c0dbcd77f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c670a9744324af2a471e23f66259238":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cc86ac421434618b6992a08bfcbdeea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbf0f2b752a14bfc8516e37ace64a4ae","placeholder":"â€‹","style":"IPY_MODEL_88ed24376dc14d48af7310d6afc61e38","value":"iglu-2022-nlp-task-states-v1.0.zip: 100%"}},"3eb928cfce2540f1b04691d64748b42a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f5db8fe5f0b40ab85f5bf527f6f6029":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f87ab37f48c4ea699466e0a244a3df8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4471b53588f54822abe07419be2fc95e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd43d555a10340ac8c5eff9535474501","IPY_MODEL_d578e757c116407cae78f9b72885258c","IPY_MODEL_7997b15718674b6a81f1306e25177e55"],"layout":"IPY_MODEL_e6e23b79a2bb49b5ab83221aa0525b3c"}},"452bb88a5ea546f59b36f1c097cbb0df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"454945d525b8452e82c50882b1e409b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4563aede5c6c4cf78dd25b934c846bba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4654fc75725d4265bc5665cc11dda1bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"467186daaa3047cd9c26b235f50feed0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b1fbe85127d4fce871b4c0f458d4caf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d726c20b8f644fa4ac5377947858f6b4","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f26d3641d21545868b01052ff2da32d6","value":231508}},"5277ae1ae0a943a0bdbaccc74336682e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"536dcbf35dd74c4aafd518e4aad725fe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5419b989589044ffae226242e0bbd2e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57325d7ad38a495b892388d2945d5f5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61dc9814bbfd4e81ada1e57a6794e5d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f321bece4d54226ad3db809bde4cd4e","IPY_MODEL_4b1fbe85127d4fce871b4c0f458d4caf","IPY_MODEL_88d292e97b534caf9838af5fabd8e540"],"layout":"IPY_MODEL_8ae76991239d410cacddb929c1d735df"}},"623a01dafad04664b67ebc194c987a12":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62cda69ba3ad4d009dea8d7e6b6f5dfc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"640be1c0eea740cea0d7c34b8bab8d0f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ca9ee7c77c545c58b7dfac7a0a33a5e","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36582915552c44f4a18fc221b0ebd6a0","value":363}},"65762849a3354fd49e60a87c13876011":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a599c3bee5b4d038c50f768119e8681","IPY_MODEL_af1ac454c7d74b3c9006232fbe9b6c70","IPY_MODEL_a375112690b541cf8029a582835f5e1d"],"layout":"IPY_MODEL_3c670a9744324af2a471e23f66259238"}},"6646173a780f46c6b7e9c356762c09bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67ba0ca897d64f07885d8deffea6e33f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_158c3bef95684e64a15dc20cbc0e6883","placeholder":"â€‹","style":"IPY_MODEL_0fe02fa28f11441fa2e56b8a50e5bb97","value":"Downloading tokenizer_config.json: 100%"}},"6f321bece4d54226ad3db809bde4cd4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17d2a8fdb0ce42089433a0619b8bc377","placeholder":"â€‹","style":"IPY_MODEL_a5fd56d0815149ab9fba81694ee2abc0","value":"Downloading vocab.txt: 100%"}},"6fcb92e57cc84dd7bb4ba3d42b9e534d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_452bb88a5ea546f59b36f1c097cbb0df","placeholder":"â€‹","style":"IPY_MODEL_cd5d95cdd8124b21ae151d6df4686215","value":"question_bank.csv: 100%"}},"71eaeb7b538c4181bcecd1e1ce00ed2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"738bf82d43f4446cbfa7548fdb3d45bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c46ff68ce34edc907309a92c0b0c54":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"767c97ecb8974a5db20253f46320560a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7997b15718674b6a81f1306e25177e55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b8235324e9043849ae3ce48310a10e0","placeholder":"â€‹","style":"IPY_MODEL_5419b989589044ffae226242e0bbd2e2","value":" 5803/5803 [00:04&lt;00:00, 1421.25it/s]"}},"7c8eae5b06ec43118f99a850374175ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80e271febd6247a9843625c81a4619d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcc0708d1d7446ba986efee780d800d4","placeholder":"â€‹","style":"IPY_MODEL_aec263264fcd4a8da5baa9e60dbf7597","value":"Train epoch 2/2: 100%"}},"87ffed7128cc4a299fa0df1cf7a816bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"881bbeb5c4ab482a91c2160db4e4e88a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd0af9a490d14ca8bb205e321314b859","placeholder":"â€‹","style":"IPY_MODEL_e47cb091546d4c2cacae5f86e491197d","value":"Tokenizing data: 100%"}},"88d292e97b534caf9838af5fabd8e540":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d8d70b4fde84533b467778c64ee2925","placeholder":"â€‹","style":"IPY_MODEL_3b22025d2e034bf290207b9c0dbcd77f","value":" 226k/226k [00:00&lt;00:00, 842kB/s]"}},"88ed24376dc14d48af7310d6afc61e38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89887da51a1d444dabb7ca8c95175ee0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ae76991239d410cacddb929c1d735df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b1ee6a872314832a4095db0bf22f370":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"956d42504a0f45bd8413876015a7e2fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a681a674ce8461682973e58a2ab36e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a72cc40690d43a0bd5bfcb7aaac10b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_467186daaa3047cd9c26b235f50feed0","max":60347,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e65332aefc4a477d9e12605de3e112a1","value":60347}},"9c4af82f506946cd880d337358516831":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d8d70b4fde84533b467778c64ee2925":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a06be6d23355447698547fb272aa1fce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1996c6abdb54ff49bdfc41077d34ba8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3eb928cfce2540f1b04691d64748b42a","placeholder":"â€‹","style":"IPY_MODEL_5277ae1ae0a943a0bdbaccc74336682e","value":"Downloading pytorch_model.bin: 100%"}},"a210597827134638bb830849be952b8c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a375112690b541cf8029a582835f5e1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_738bf82d43f4446cbfa7548fdb3d45bc","placeholder":"â€‹","style":"IPY_MODEL_152583b8434044029fc3dd8c3b01019e","value":" 65/65 [00:07&lt;00:00,  8.81it/s]"}},"a39165728b2245cbad191c61418a4ab5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7ae34cd46ca4509bd38404c3daa3538","max":1025,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4281ccb093047d8af28894544ef80de","value":1025}},"a43d6f520fe441ec94ed76864e6ad686":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5fd56d0815149ab9fba81694ee2abc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6ab9a7889854fcf88419b43b1861421":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a76dabd8a6e1461199d860aa7c9d72b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1996c6abdb54ff49bdfc41077d34ba8","IPY_MODEL_ed46f11a132a4cf18d34a5e97d6fc31a","IPY_MODEL_2b725e64b982402cbd179f0708a649d8"],"layout":"IPY_MODEL_57325d7ad38a495b892388d2945d5f5c"}},"aaf912d265f3441094e1259a49c9cf63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ae445705931a41ec80ed1047672628f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aec263264fcd4a8da5baa9e60dbf7597":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af1ac454c7d74b3c9006232fbe9b6c70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee300a5b92cd4eb1bef0a54a268bd52f","max":65,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aaf912d265f3441094e1259a49c9cf63","value":65}},"b4281ccb093047d8af28894544ef80de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5a61fafc66e4943b98ed21973f81e4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b78204b1a52341239f8bb485c1444847":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8a99cf31b4c49e9bb1a46689c2a87c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80e271febd6247a9843625c81a4619d5","IPY_MODEL_2e2a37ded05a402890e9188bbfde6f03","IPY_MODEL_c2390716a10d483f9c592291c4bd2e64"],"layout":"IPY_MODEL_f9b26663c6564e4782975b49210974c4"}},"b8c0d9dc44164f90b50d43125d3bef3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71eaeb7b538c4181bcecd1e1ce00ed2d","max":13851855,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6ab9a7889854fcf88419b43b1861421","value":13851855}},"bb415c3be154471fb714854f59da1d92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd0af9a490d14ca8bb205e321314b859":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c13ccff919a6467a9bda5a58f60d808c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38a3616001514d409e8589fa51545fee","placeholder":"â€‹","style":"IPY_MODEL_03db55e7ae8b44a591634f9edc458624","value":" 13.9M/13.9M [00:00&lt;00:00, 14.7MB/s]"}},"c2390716a10d483f9c592291c4bd2e64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62cda69ba3ad4d009dea8d7e6b6f5dfc","placeholder":"â€‹","style":"IPY_MODEL_1eeb928899284259a2a28bdeb7ddc7e1","value":" 363/363 [02:06&lt;00:00,  3.10it/s]"}},"c79348e0c978443794c9b83d476a4a1f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca45f1604d524275a6b78eae2b028f56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cc86ac421434618b6992a08bfcbdeea","IPY_MODEL_b8c0d9dc44164f90b50d43125d3bef3c","IPY_MODEL_c13ccff919a6467a9bda5a58f60d808c"],"layout":"IPY_MODEL_8b1ee6a872314832a4095db0bf22f370"}},"cd43d555a10340ac8c5eff9535474501":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c79348e0c978443794c9b83d476a4a1f","placeholder":"â€‹","style":"IPY_MODEL_330e9455dae8406ea6cf48463b4bd373","value":"Tokenizing data: 100%"}},"cd5d95cdd8124b21ae151d6df4686215":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4a7e94247d34e98b09686a425ca946c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6997bb0ab9b45338f99f97968c035a7","IPY_MODEL_f96be4566e8441399e1a9a21755bfdf6","IPY_MODEL_1876f748227547aba4398b1e4b399199"],"layout":"IPY_MODEL_623a01dafad04664b67ebc194c987a12"}},"d578e757c116407cae78f9b72885258c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_536dcbf35dd74c4aafd518e4aad725fe","max":5803,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35fa9d72c9ac488892631ced49b6a08e","value":5803}},"d726c20b8f644fa4ac5377947858f6b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8b2c8a3b51849888f0f9d9abf553874":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc2c9640e77a4986ace4b4e00488f030":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcc0708d1d7446ba986efee780d800d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de03f9e19942488e924f21dd71dd7d55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea6844d6ff3747b18e19412d89aa6579","IPY_MODEL_640be1c0eea740cea0d7c34b8bab8d0f","IPY_MODEL_334e3c7a448948e09ca448c5d7cdf442"],"layout":"IPY_MODEL_efb5c3f547c0409f96c953e9f0dcd34d"}},"e05db359d2f14b8e9f1a9bc2e467d9da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fcb92e57cc84dd7bb4ba3d42b9e534d","IPY_MODEL_9a72cc40690d43a0bd5bfcb7aaac10b0","IPY_MODEL_36bc4af466ea486ab0b9e30da832007a"],"layout":"IPY_MODEL_03a618ff4ee0401f957d85497747bfdd"}},"e3011a737a594d1f9ddcf1b43fed5f6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_881bbeb5c4ab482a91c2160db4e4e88a","IPY_MODEL_a39165728b2245cbad191c61418a4ab5","IPY_MODEL_e988e0b8cd524b01b31a6b2166b6e046"],"layout":"IPY_MODEL_74c46ff68ce34edc907309a92c0b0c54"}},"e3d9f3b105854da1a79aae69cfa55f19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_282675c3d54e4c7f97bf688a12eedd34","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_767c97ecb8974a5db20253f46320560a","value":28}},"e401b29f14f247e1b649df1ba8779dd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecb8f98acf3a4bc6b3b8e2120bc7c33d","placeholder":"â€‹","style":"IPY_MODEL_4654fc75725d4265bc5665cc11dda1bc","value":"clarifying_questions_train.csv: 100%"}},"e47cb091546d4c2cacae5f86e491197d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e65332aefc4a477d9e12605de3e112a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6997bb0ab9b45338f99f97968c035a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a210597827134638bb830849be952b8c","placeholder":"â€‹","style":"IPY_MODEL_b5a61fafc66e4943b98ed21973f81e4f","value":"Downloading config.json: 100%"}},"e6e23b79a2bb49b5ab83221aa0525b3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e708cadd2e4a405e8adcb297182ac88a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7ae34cd46ca4509bd38404c3daa3538":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e869f95a3a674aea98c1e0c8835a6bf2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e940e19e08154c09b9c789db00fee1f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a681a674ce8461682973e58a2ab36e7","placeholder":"â€‹","style":"IPY_MODEL_217a147af5e4480bb42e7aaea92b0687","value":" 2.55M/2.55M [00:00&lt;00:00, 3.55MB/s]"}},"e988e0b8cd524b01b31a6b2166b6e046":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4563aede5c6c4cf78dd25b934c846bba","placeholder":"â€‹","style":"IPY_MODEL_fdedfcc0b56c40b3826f46da2a5f5597","value":" 1025/1025 [00:00&lt;00:00, 1263.67it/s]"}},"ea6844d6ff3747b18e19412d89aa6579":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f5db8fe5f0b40ab85f5bf527f6f6029","placeholder":"â€‹","style":"IPY_MODEL_87ffed7128cc4a299fa0df1cf7a816bb","value":"Train epoch 1/2: 100%"}},"ea7730e4d9ea4a7bbc1cf2572434549e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b78204b1a52341239f8bb485c1444847","max":2551380,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6646173a780f46c6b7e9c356762c09bd","value":2551380}},"ec0ac8e334864dde9c94ec8ae5289635":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ecb8f98acf3a4bc6b3b8e2120bc7c33d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed46f11a132a4cf18d34a5e97d6fc31a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_edeb4a20cb4144d78103e62481f33f01","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_956d42504a0f45bd8413876015a7e2fc","value":440473133}},"edeb4a20cb4144d78103e62481f33f01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee300a5b92cd4eb1bef0a54a268bd52f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efb5c3f547c0409f96c953e9f0dcd34d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f26d3641d21545868b01052ff2da32d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f46af155d2914e4d9f062f51d869d1e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67ba0ca897d64f07885d8deffea6e33f","IPY_MODEL_e3d9f3b105854da1a79aae69cfa55f19","IPY_MODEL_2bfe0f794fe144aa81187c4affe9f7ab"],"layout":"IPY_MODEL_a43d6f520fe441ec94ed76864e6ad686"}},"f96be4566e8441399e1a9a21755bfdf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e869f95a3a674aea98c1e0c8835a6bf2","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb415c3be154471fb714854f59da1d92","value":570}},"f9b26663c6564e4782975b49210974c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbf0f2b752a14bfc8516e37ace64a4ae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdedfcc0b56c40b3826f46da2a5f5597":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
